{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Assuming you have installed 'unsloth' and 'trl'\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# File that contains the 100 multiple-choice questions\n",
    "TEST_FILE = \"psychology_test_multiple_choice.json\"\n",
    "\n",
    "# Define a multiple-choice chat prompt\n",
    "# The prompt instructs the model to respond with a single correct letter.\n",
    "chat_prompt = \"\"\"\\\n",
    "### Instruction:\n",
    "You are a helpful AI assistant. Read the multiple choice question and options below. \n",
    "Provide only the single best answer (A, B, C, or D). Just giving me an alphabet is enough.\n",
    "Example is this.\n",
    "Response: A, B, C, or D\n",
    "\n",
    "### Input:\n",
    "Question: {question}\n",
    "Options:\n",
    "{options_str}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def load_test_data(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_letter_from_response(text):\n",
    "    # Simple approach: search for an isolated letter A-D\n",
    "    match = re.search(r\"\\b[ABCD]\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return None\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, device=\"cuda\", model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on multiple-choice items.\n",
    "    Returns the accuracy (fraction correct).\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "    invalid_responses = 0\n",
    "    \n",
    "    for item in test_data:\n",
    "        q = item[\"question\"]\n",
    "        opts = \"\\n\".join(item[\"options\"])\n",
    "        true_answer = item[\"answer\"].upper()\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = chat_prompt.format(question=q, options_str=opts)\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded\n",
    "\n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1  # Count invalid responses\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    # print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model...\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Evaluating original model...\n",
      "[Original Model] Accuracy: 47.00%  (47/100)\n",
      "[Original Model] Invalid Responses: 7/100 (7.00%)\n",
      "\n",
      "Loading fine-tuned model (LoRA)...\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Evaluating fine-tuned model...\n",
      "[Fine-Tuned Model] Accuracy: 67.00%  (67/100)\n",
      "[Fine-Tuned Model] Invalid Responses: 5/100 (5.00%)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load test data\n",
    "    test_data = load_test_data(TEST_FILE)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(\"Loading original model...\")\n",
    "    original_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "\n",
    "    max_seq_length = 2048\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "\n",
    "    orig_model, orig_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=original_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "    # Prepare it for inference (disable training settings)\n",
    "    FastLanguageModel.for_inference(orig_model)\n",
    "    orig_model.to(device)\n",
    "\n",
    "    # Evaluate original model\n",
    "    print(\"Evaluating original model...\")\n",
    "    evaluate_model(orig_model, orig_tokenizer, test_data, device=device, model_name=\"Original Model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2) Load the FINE-TUNED model\n",
    "    # ----------------------------------------------------------------------\n",
    "    # This folder \"lora_model_osloth\" is whatever you saved from your fine-tuning.\n",
    "    # Make sure it matches your actual path.\n",
    "    print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "    fine_tuned_model_name = \"lora_model_osloth_psychology\"\n",
    "\n",
    "    finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=fine_tuned_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "    # Prepare for inference\n",
    "    FastLanguageModel.for_inference(finetune_model)\n",
    "    finetune_model.to(device)\n",
    "\n",
    "    # Evaluate fine-tuned model\n",
    "    print(\"Evaluating fine-tuned model...\")\n",
    "    evaluate_model(finetune_model, finetune_tokenizer, test_data, device=device, model_name=\"Fine-Tuned Model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model...\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_test_data(TEST_FILE)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Loading original model...\")\n",
    "original_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "orig_model, orig_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=original_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(orig_model)\n",
    "orig_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: D\n",
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: D\n",
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: C\n"
     ]
    }
   ],
   "source": [
    "for item in test_data[:30]:\n",
    "    q = item[\"question\"]\n",
    "    opts = \"\\n\".join(item[\"options\"])\n",
    "    true_answer = item[\"answer\"].upper()\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = chat_prompt.format(question=q, options_str=opts)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = orig_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = orig_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            use_cache=True\n",
    "        )\n",
    "    # Decode\n",
    "    decoded = orig_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"### Response:\" in decoded:\n",
    "        model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        model_response = decoded\n",
    "\n",
    "    predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "    if predicted_letter != true_answer:\n",
    "        print(\"==========================\")\n",
    "        print(f\"Correct Answer: {true_answer}\")\n",
    "        print(f\"Predicted letter: {predicted_letter}\")\n",
    "        # print(f\"Decoded Text: {decoded}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating original model...\")\n",
    "evaluate_model(orig_model, orig_tokenizer, test_data, device=device, model_name=\"Original Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading fine-tuned model (LoRA)...\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "fine_tuned_model_name = \"lora_model_osloth\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Prepare for inference\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: B\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: None\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: A\n"
     ]
    }
   ],
   "source": [
    "for item in test_data[:30]:\n",
    "    q = item[\"question\"]\n",
    "    opts = \"\\n\".join(item[\"options\"])\n",
    "    true_answer = item[\"answer\"].upper()\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = chat_prompt.format(question=q, options_str=opts)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = finetune_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = finetune_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            use_cache=True\n",
    "        )\n",
    "    # Decode\n",
    "    decoded = finetune_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"### Response:\" in decoded:\n",
    "        model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        model_response = decoded\n",
    "\n",
    "    predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "    if predicted_letter != true_answer:\n",
    "        print(\"==========================\")\n",
    "        print(f\"Correct Answer: {true_answer}\")\n",
    "        print(f\"Predicted letter: {predicted_letter}\")\n",
    "        # print(f\"Decoded Text: {decoded}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model\n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "evaluate_model(finetune_model, finetune_tokenizer, test_data, device=device, model_name=\"Fine-Tuned Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
