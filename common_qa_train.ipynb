{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from utils import preprocess_qa, RestrictToValidTokens\n",
    "pd.options.display.max_colwidth = None\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "hf_auth_token = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "ds = load_dataset(\"tau/commonsense_qa\")\n",
    "trained_folder = \"./fine-tuned/llama-qa-lora_overfit_100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds['train'])\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f3ff5763e04fe89ebeb2f50573d859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=6.0  # Adjust threshold for higher precision on sensitive layers\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    use_auth_token=hf_auth_token,\n",
    "    cache_dir=\"/fs03/yu60/kojitanaka/model_cache\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    use_auth_token=hf_auth_token,\n",
    "    cache_dir=\"/fs03/yu60/kojitanaka/model_cache\",\n",
    "    device_map=\"auto\",  # Automatically maps layers to GPU\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add a new pad token\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "# 2. Resize model embeddings to match the new (larger) vocabulary\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# 3. Set pad token + pad_token_id\n",
    "tokenizer.pad_token = \"<|pad|>\"\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<|pad|>\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "max_length = min(tokenizer.model_max_length, 256)\n",
    "\n",
    "def create_tokenized_ds_for_finetune(example):\n",
    "    prompt_text = preprocess_qa(example)['text']\n",
    "\n",
    "    tokenized_prompt = tokenizer(prompt_text, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=max_length)\n",
    "\n",
    "    answer_token = tokenizer(example['answerKey'].strip(), return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    input_ids = tokenized_prompt[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = tokenized_prompt[\"attention_mask\"].squeeze(0)\n",
    "    answer_input_ids = answer_token[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    labels = torch.full_like(input_ids, -100)\n",
    "    if answer_input_ids.numel() == 1:\n",
    "        next_pos = input_ids.ne(tokenizer.pad_token_id).sum()\n",
    "        labels[next_pos] = answer_input_ids.item()\n",
    "    else:\n",
    "        start_pos = input_ids.ne(tokenizer.pad_token_id).sum()\n",
    "        labels[start_pos : start_pos + answer_input_ids.size(0)] = answer_input_ids\n",
    "\n",
    "    pad_length = max_length - input_ids.shape[0]\n",
    "    \n",
    "    if pad_length > 0:\n",
    "        input_ids = torch.cat([input_ids, torch.full((pad_length,), tokenizer.pad_token_id)])\n",
    "        attention_mask = torch.cat([attention_mask, torch.zeros(pad_length, dtype=torch.long)])\n",
    "        labels = torch.cat([labels, torch.full((pad_length,), -100)])\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "tokenized_ds_finetune = ds.map(create_tokenized_ds_for_finetune, remove_columns=ds['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length Used: 256\n",
      "\n",
      "Example 1:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?. Options: A: ignore B: enforce C: authoritarian D: yell at E: avoid. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: A\n",
      "\n",
      "Example 2:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: Sammy wanted to go to where the people were.  Where might he go?. Options: A: race track B: populated areas C: the desert D: apartment E: roadblock. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: B\n",
      "\n",
      "Example 3:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: To locate a choker not located in a jewelry box or boutique where would you go?. Options: A: jewelry store B: neck C: jewlery box D: jewelry box E: boutique. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: A\n"
     ]
    }
   ],
   "source": [
    "# Print the max length and verify sequence lengths\n",
    "print(f\"Max Length Used: {max_length}\")\n",
    "\n",
    "# Iterate correctly through the dataset\n",
    "for idx in range(3):  # Use indexing directly to access each item\n",
    "    example = tokenized_ds_finetune['train'][idx]  # Accessing each dictionary correctly\n",
    "    input_ids = example['input_ids']\n",
    "    labels = example['labels']\n",
    "\n",
    "    # Print lengths and ensure the types are correct\n",
    "    print(f\"\\nExample {idx + 1}:\")\n",
    "    print(\"Input Length:\", len(input_ids))  \n",
    "    print(\"Labels Length:\", len(labels))\n",
    "    \n",
    "    # Decode the tokens and print (only unmasked tokens for labels)\n",
    "    print(\"Input Tokens Decoded:\", tokenizer.decode(input_ids))\n",
    "    print(\"Labels Tokens Decoded:\", tokenizer.decode([t for t in labels if t != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",      \n",
    "    lora_dropout=0.0,     \n",
    "    task_type=TaskType.CAUSAL_LM  \n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_ds_finetune[\"train\"].select(range(16))\n",
    "validation_dataset = tokenized_ds_finetune[\"validation\"].select(range(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2867889/475297386.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>14.378200</td>\n",
       "      <td>14.269659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.113100</td>\n",
       "      <td>11.523464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.388100</td>\n",
       "      <td>9.988491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.907600</td>\n",
       "      <td>7.652774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.811500</td>\n",
       "      <td>5.934968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.082200</td>\n",
       "      <td>4.732222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.561400</td>\n",
       "      <td>3.871064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.524100</td>\n",
       "      <td>3.319704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.360200</td>\n",
       "      <td>2.994216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.197500</td>\n",
       "      <td>2.872624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned/llama-qa-lora_overfit_4/tokenizer_config.json',\n",
       " './fine-tuned/llama-qa-lora_overfit_4/special_tokens_map.json',\n",
       " './fine-tuned/llama-qa-lora_overfit_4/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))  \n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = \"<|pad|>\"\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<|pad|>\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=trained_folder,     \n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    save_strategy=\"epoch\",           \n",
    "    learning_rate=3e-5,               \n",
    "    per_device_train_batch_size=4,    \n",
    "    per_device_eval_batch_size=4,    \n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,                \n",
    "    logging_dir=\"./logs\",            \n",
    "    logging_steps=1,                \n",
    "    save_total_limit=3,              \n",
    "    load_best_model_at_end=True,     \n",
    "    fp16=False,                       \n",
    "    gradient_accumulation_steps=1,   \n",
    "    report_to=\"none\"                 \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(trained_folder)\n",
    "tokenizer.save_pretrained(trained_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
