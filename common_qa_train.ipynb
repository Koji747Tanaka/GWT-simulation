{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "import os\n",
    "import bitsandbytes as bnb  # For 8-bit quantization\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from utils import preprocess_qa, RestrictToValidTokens\n",
    "pd.options.display.max_colwidth = None\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "hf_auth_token = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "ds = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_concept</th>\n",
       "      <th>choices</th>\n",
       "      <th>answerKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>075e483d21c29a511267ef62bedc0461</td>\n",
       "      <td>The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?</td>\n",
       "      <td>punishing</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61fe6e879ff18686d7552425a36344c8</td>\n",
       "      <td>Sammy wanted to go to where the people were.  Where might he go?</td>\n",
       "      <td>people</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['race track', 'populated areas', 'the desert', 'apartment', 'roadblock']}</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c1cb0e95b99f72d55c068ba0255c54d</td>\n",
       "      <td>To locate a choker not located in a jewelry box or boutique where would you go?</td>\n",
       "      <td>choker</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['jewelry store', 'neck', 'jewlery box', 'jewelry box', 'boutique']}</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02e821a3e53cb320790950aab4489e85</td>\n",
       "      <td>Google Maps and other highway and street GPS services have replaced what?</td>\n",
       "      <td>highway</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['united states', 'mexico', 'countryside', 'atlas', 'oceans']}</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23505889b94e880c3e89cff4ba119860</td>\n",
       "      <td>The fox walked from the city into the forest, what was it looking for?</td>\n",
       "      <td>fox</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['pretty flowers.', 'hen house', 'natural habitat', 'storybook', 'dense forest']}</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  075e483d21c29a511267ef62bedc0461   \n",
       "1  61fe6e879ff18686d7552425a36344c8   \n",
       "2  4c1cb0e95b99f72d55c068ba0255c54d   \n",
       "3  02e821a3e53cb320790950aab4489e85   \n",
       "4  23505889b94e880c3e89cff4ba119860   \n",
       "\n",
       "                                                                                                                     question  \\\n",
       "0  The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?   \n",
       "1                                                            Sammy wanted to go to where the people were.  Where might he go?   \n",
       "2                                             To locate a choker not located in a jewelry box or boutique where would you go?   \n",
       "3                                                   Google Maps and other highway and street GPS services have replaced what?   \n",
       "4                                                      The fox walked from the city into the forest, what was it looking for?   \n",
       "\n",
       "  question_concept  \\\n",
       "0        punishing   \n",
       "1           people   \n",
       "2           choker   \n",
       "3          highway   \n",
       "4              fox   \n",
       "\n",
       "                                                                                                                          choices  \\\n",
       "0                        {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}   \n",
       "1         {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['race track', 'populated areas', 'the desert', 'apartment', 'roadblock']}   \n",
       "2               {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['jewelry store', 'neck', 'jewlery box', 'jewelry box', 'boutique']}   \n",
       "3                     {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['united states', 'mexico', 'countryside', 'atlas', 'oceans']}   \n",
       "4  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['pretty flowers.', 'hen house', 'natural habitat', 'storybook', 'dense forest']}   \n",
       "\n",
       "  answerKey  \n",
       "0         A  \n",
       "1         B  \n",
       "2         A  \n",
       "3         D  \n",
       "4         C  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ds['train'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b9c2f2daef44a087d1ad2c20423f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=6.0  # Adjust threshold for higher precision on sensitive layers\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    use_auth_token=hf_auth_token,\n",
    "    cache_dir=\"/fs03/yu60/kojitanaka/model_cache\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    use_auth_token=hf_auth_token,\n",
    "    cache_dir=\"/fs03/yu60/kojitanaka/model_cache\",\n",
    "    device_map=\"auto\",  # Automatically maps layers to GPU\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder = \"./llama-qa-lora_overfit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add a new pad token\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "# 2. Resize model embeddings to match the new (larger) vocabulary\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# 3. Set pad token + pad_token_id\n",
    "tokenizer.pad_token = \"<|pad|>\"\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<|pad|>\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "max_length = min(tokenizer.model_max_length, 256)\n",
    "\n",
    "def create_tokenized_ds_for_finetune(example):\n",
    "    import torch\n",
    "\n",
    "    prompt_text = preprocess_qa(example)['text']\n",
    "\n",
    "    tokenized_prompt = tokenizer(prompt_text, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=max_length)\n",
    "\n",
    "    answer_token = tokenizer(example['answerKey'].strip(), return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    input_ids = tokenized_prompt[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = tokenized_prompt[\"attention_mask\"].squeeze(0)\n",
    "    answer_input_ids = answer_token[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    labels = torch.full_like(input_ids, -100)\n",
    "    if answer_input_ids.numel() == 1:\n",
    "        next_pos = input_ids.ne(tokenizer.pad_token_id).sum()\n",
    "        labels[next_pos] = answer_input_ids.item()\n",
    "    else:\n",
    "        start_pos = input_ids.ne(tokenizer.pad_token_id).sum()\n",
    "        labels[start_pos : start_pos + answer_input_ids.size(0)] = answer_input_ids\n",
    "\n",
    "    pad_length = max_length - input_ids.shape[0]\n",
    "    \n",
    "    if pad_length > 0:\n",
    "        input_ids = torch.cat([input_ids, torch.full((pad_length,), tokenizer.pad_token_id)])\n",
    "        attention_mask = torch.cat([attention_mask, torch.zeros(pad_length, dtype=torch.long)])\n",
    "        labels = torch.cat([labels, torch.full((pad_length,), -100)])\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "tokenized_ds_finetune = ds.map(create_tokenized_ds_for_finetune, remove_columns=ds['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length Used: 256\n",
      "\n",
      "Example 1:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Options: A: ignore B: enforce C: authoritarian D: yell at E: avoid. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: A\n",
      "\n",
      "Example 2:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: Sammy wanted to go to where the people were.  Where might he go? Options: A: race track B: populated areas C: the desert D: apartment E: roadblock. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: B\n",
      "\n",
      "Example 3:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: To locate a choker not located in a jewelry box or boutique where would you go? Options: A: jewelry store B: neck C: jewlery box D: jewelry box E: boutique. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: A\n"
     ]
    }
   ],
   "source": [
    "# Print the max length and verify sequence lengths\n",
    "print(f\"Max Length Used: {max_length}\")\n",
    "\n",
    "# Iterate correctly through the dataset\n",
    "for idx in range(3):  # Use indexing directly to access each item\n",
    "    example = tokenized_ds_finetune['train'][idx]  # Accessing each dictionary correctly\n",
    "    input_ids = example['input_ids']\n",
    "    labels = example['labels']\n",
    "\n",
    "    # Print lengths and ensure the types are correct\n",
    "    print(f\"\\nExample {idx + 1}:\")\n",
    "    print(\"Input Length:\", len(input_ids))  \n",
    "    print(\"Labels Length:\", len(labels))\n",
    "    \n",
    "    # Decode the tokens and print (only unmasked tokens for labels)\n",
    "    print(\"Input Tokens Decoded:\", tokenizer.decode(input_ids))\n",
    "    print(\"Labels Tokens Decoded:\", tokenizer.decode([t for t in labels if t != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                   # Rank for low-rank matrices\n",
    "    lora_alpha=32,         # Scaling factor for LoRA updates\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Fine-tune only query and value layers\n",
    "    bias=\"none\",      \n",
    "    lora_dropout=0.0,     \n",
    "    task_type=TaskType.CAUSAL_LM  \n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already processed tokenized_ds with text and target_text columns\n",
    "train_dataset = tokenized_ds_finetune[\"train\"].select(range(16))\n",
    "validation_dataset = tokenized_ds_finetune[\"validation\"].select(range(16))\n",
    "# train_dataset['labels']\n",
    "# print(tokenized_ds['train'][0])\n",
    "# Should show input_ids, attention_mask, and labels where only the target tokens are supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_2711282/2978538061.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 01:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.684482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.823765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.774842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.686615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.336946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.226562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.721681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.933595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.493792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.276869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./llama-qa-lora_overfit/tokenizer_config.json',\n",
       " './llama-qa-lora_overfit/special_tokens_map.json',\n",
       " './llama-qa-lora_overfit/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))  \n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = \"<|pad|>\"\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<|pad|>\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=trained_folder,     \n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    save_strategy=\"epoch\",           \n",
    "    learning_rate=3e-5,               \n",
    "    per_device_train_batch_size=1,    \n",
    "    per_device_eval_batch_size=1,    \n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,                \n",
    "    logging_dir=\"./logs\",            \n",
    "    logging_steps=1,                \n",
    "    save_total_limit=3,              \n",
    "    load_best_model_at_end=True,     \n",
    "    fp16=False,                       \n",
    "    gradient_accumulation_steps=1,   \n",
    "    report_to=\"none\"                 \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(trained_folder)\n",
    "tokenizer.save_pretrained(trained_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: D, Ground Truth: A\n",
      "Prediction: E, Ground Truth: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: E, Ground Truth: A\n",
      "Prediction: A, Ground Truth: D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: A, Ground Truth: C\n",
      "Prediction: A, Ground Truth: D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: B, Ground Truth: E\n",
      "Prediction: E, Ground Truth: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: D, Ground Truth: E\n",
      "Prediction: A, Ground Truth: D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: E, Ground Truth: B\n",
      "Prediction: A, Ground Truth: C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: E, Ground Truth: C\n",
      "Prediction: A, Ground Truth: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: E, Ground Truth: C\n",
      "Prediction: A, Ground Truth: D\n",
      "\n",
      "Accuracy: 6.25%\n"
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList, LogitsProcessor\n",
    "# Custom LogitsProcessor for valid tokens restriction\n",
    "class RestrictToValidTokens(LogitsProcessor):\n",
    "    def __init__(self, valid_tokens):\n",
    "        self.valid_tokens = valid_tokens\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Mask all logits except the valid tokens\n",
    "        mask = torch.full_like(scores, float('-inf'))\n",
    "        mask[..., self.valid_tokens] = 0\n",
    "        return scores + mask\n",
    "\n",
    "# Convert valid tokens to token IDs\n",
    "valid_tokens = [tokenizer.convert_tokens_to_ids(token) for token in ['A', 'B', 'C', 'D', 'E']]\n",
    "\n",
    "# Instantiate the logits processor with valid tokens\n",
    "logits_processor = LogitsProcessorList([RestrictToValidTokens(valid_tokens)])\n",
    "\n",
    "correct_predictions = 0\n",
    "for idx in range(16):\n",
    "    example = train_dataset[idx]  # Ensure using the same preprocessed dataset\n",
    "    inputs = {k: torch.tensor(v).unsqueeze(0).to(model.device) for k, v in example.items() if k != \"labels\"}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, \n",
    "                                 max_new_tokens=1, \n",
    "                                 eos_token_id=tokenizer.eos_token_id, \n",
    "                                 top_k=5,\n",
    "                                 logits_processor=logits_processor\n",
    "                                 )\n",
    "        generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        prediction_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Decode ground truth from tokenized labels\n",
    "        ground_truth = tokenizer.decode([token for token in example['labels'] if token != -100])\n",
    "        \n",
    "        if prediction_text.strip() == ground_truth.strip():\n",
    "            correct_predictions += 1\n",
    "\n",
    "        print(f\"Prediction: {prediction_text}, Ground Truth: {ground_truth}\")\n",
    "\n",
    "accuracy = correct_predictions / 16 * 100\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenized_ds[\"train\"][0]\n",
    "print(\"Input IDs:\", example[\"input_ids\"])\n",
    "print(\"Labels:   \", example[\"labels\"])\n",
    "\n",
    "print(\"Decoded Input:\", tokenizer.decode(example[\"input_ids\"]))\n",
    "print(\"Decoded Label:\", tokenizer.decode([x for x in example[\"labels\"] if x != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "example = tokenized_ds[\"train\"][0]\n",
    "input_ids = torch.tensor(example[\"input_ids\"])\n",
    "labels = torch.tensor(example[\"labels\"])\n",
    "\n",
    "label_positions = (labels != -100).nonzero(as_tuple=True)[0]\n",
    "print(\"Label positions:\", label_positions.tolist())\n",
    "\n",
    "non_pad_count = int((input_ids != tokenizer.pad_token_id).sum())\n",
    "print(\"Non-pad count:\", non_pad_count)\n",
    "\n",
    "# Compare them\n",
    "print(\"Decoded input:\", tokenizer.decode(input_ids))\n",
    "print(\"Decoded label:\", tokenizer.decode(labels[labels != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.train()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Ensure tensors are properly shaped and moved to the correct device\n",
    "for k in batch:\n",
    "    batch[k] = torch.tensor(batch[k]).unsqueeze(0).to(model.device)  # Adding batch dimension\n",
    "\n",
    "# Forward pass in training mode\n",
    "outputs = model(**batch)\n",
    "print(\"Manual training-mode loss:\", outputs.loss.item())\n",
    "\n",
    "# Forward pass in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_eval = model(**batch)\n",
    "print(\"Manual eval-mode loss:\", outputs_eval.loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exactly how many labels are non-masked\n",
    "for idx in range(3):\n",
    "    example = train_dataset[idx]\n",
    "    input_ids = torch.tensor(example['input_ids'])\n",
    "    labels = torch.tensor(example['labels'])\n",
    "    non_pad_tokens = (labels != -100).sum().item()\n",
    "    print(f\"Example {idx}: Non-Pad Tokens: {non_pad_tokens}, Label Positions:\", (labels != -100).nonzero(as_tuple=True)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
