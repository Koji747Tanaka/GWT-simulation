{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from utils import preprocess_qa, RestrictToValidTokens\n",
    "pd.options.display.max_colwidth = None\n",
    "hf_auth_token = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "ds = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder = \"./fine-tuned/llama-qa-lora_16\"\n",
    "\n",
    "# bnb config\n",
    "llm_int8_threshold = 6.0\n",
    "\n",
    "# Lora Config\n",
    "rank = 8\n",
    "lora_alpha = 32\n",
    "target_modules = [\"q_proj\", \"v_proj\"] # [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], [\"q_proj\", \"v_proj\", \"k_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] \n",
    "bias = 'none'\n",
    "lora_dropout = 0.0\n",
    "\n",
    "# Training\n",
    "training_size = 16\n",
    "validation_size = 16\n",
    "\n",
    "# Learning\n",
    "learning_rate = 3e-5\n",
    "train_batch_size = 1\n",
    "eval_batch_size = 1\n",
    "train_epochs = 10\n",
    "weight_decay= 0.01\n",
    "is_fp16=False\n",
    "gradient_accumulation_steps=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_concept</th>\n",
       "      <th>choices</th>\n",
       "      <th>answerKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>075e483d21c29a511267ef62bedc0461</td>\n",
       "      <td>The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?</td>\n",
       "      <td>punishing</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61fe6e879ff18686d7552425a36344c8</td>\n",
       "      <td>Sammy wanted to go to where the people were.  Where might he go?</td>\n",
       "      <td>people</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['race track', 'populated areas', 'the desert', 'apartment', 'roadblock']}</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c1cb0e95b99f72d55c068ba0255c54d</td>\n",
       "      <td>To locate a choker not located in a jewelry box or boutique where would you go?</td>\n",
       "      <td>choker</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['jewelry store', 'neck', 'jewlery box', 'jewelry box', 'boutique']}</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02e821a3e53cb320790950aab4489e85</td>\n",
       "      <td>Google Maps and other highway and street GPS services have replaced what?</td>\n",
       "      <td>highway</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['united states', 'mexico', 'countryside', 'atlas', 'oceans']}</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23505889b94e880c3e89cff4ba119860</td>\n",
       "      <td>The fox walked from the city into the forest, what was it looking for?</td>\n",
       "      <td>fox</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['pretty flowers.', 'hen house', 'natural habitat', 'storybook', 'dense forest']}</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  075e483d21c29a511267ef62bedc0461   \n",
       "1  61fe6e879ff18686d7552425a36344c8   \n",
       "2  4c1cb0e95b99f72d55c068ba0255c54d   \n",
       "3  02e821a3e53cb320790950aab4489e85   \n",
       "4  23505889b94e880c3e89cff4ba119860   \n",
       "\n",
       "                                                                                                                     question  \\\n",
       "0  The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?   \n",
       "1                                                            Sammy wanted to go to where the people were.  Where might he go?   \n",
       "2                                             To locate a choker not located in a jewelry box or boutique where would you go?   \n",
       "3                                                   Google Maps and other highway and street GPS services have replaced what?   \n",
       "4                                                      The fox walked from the city into the forest, what was it looking for?   \n",
       "\n",
       "  question_concept  \\\n",
       "0        punishing   \n",
       "1           people   \n",
       "2           choker   \n",
       "3          highway   \n",
       "4              fox   \n",
       "\n",
       "                                                                                                                          choices  \\\n",
       "0                        {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}   \n",
       "1         {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['race track', 'populated areas', 'the desert', 'apartment', 'roadblock']}   \n",
       "2               {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['jewelry store', 'neck', 'jewlery box', 'jewelry box', 'boutique']}   \n",
       "3                     {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['united states', 'mexico', 'countryside', 'atlas', 'oceans']}   \n",
       "4  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['pretty flowers.', 'hen house', 'natural habitat', 'storybook', 'dense forest']}   \n",
       "\n",
       "  answerKey  \n",
       "0         A  \n",
       "1         B  \n",
       "2         A  \n",
       "3         D  \n",
       "4         C  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ds['train'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811bb2d1526540228c98f874493196eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=llm_int8_threshold  # Adjust threshold for higher precision on sensitive layers\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    use_auth_token=hf_auth_token,\n",
    "    cache_dir=\"/fs03/yu60/kojitanaka/model_cache\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    use_auth_token=hf_auth_token,\n",
    "    cache_dir=\"/fs03/yu60/kojitanaka/model_cache\",\n",
    "    device_map=\"auto\",  # Automatically maps layers to GPU\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add a new pad token\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "# 2. Resize model embeddings to match the new (larger) vocabulary\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# 3. Set pad token + pad_token_id\n",
    "tokenizer.pad_token = \"<|pad|>\"\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<|pad|>\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "max_length = min(tokenizer.model_max_length, 256)\n",
    "\n",
    "def create_tokenized_ds_for_finetune(example):\n",
    "    prompt_text = preprocess_qa(example)['text']\n",
    "\n",
    "    tokenized_prompt = tokenizer(prompt_text, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=max_length)\n",
    "\n",
    "    answer_token = tokenizer(example['answerKey'].strip(), return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    input_ids = tokenized_prompt[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = tokenized_prompt[\"attention_mask\"].squeeze(0)\n",
    "    answer_input_ids = answer_token[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    labels = torch.full_like(input_ids, -100)\n",
    "    if answer_input_ids.numel() == 1:\n",
    "        next_pos = input_ids.ne(tokenizer.pad_token_id).sum()\n",
    "        labels[next_pos] = answer_input_ids.item()\n",
    "    else:\n",
    "        start_pos = input_ids.ne(tokenizer.pad_token_id).sum()\n",
    "        labels[start_pos : start_pos + answer_input_ids.size(0)] = answer_input_ids\n",
    "\n",
    "    pad_length = max_length - input_ids.shape[0]\n",
    "    \n",
    "    if pad_length > 0:\n",
    "        input_ids = torch.cat([input_ids, torch.full((pad_length,), tokenizer.pad_token_id)])\n",
    "        attention_mask = torch.cat([attention_mask, torch.zeros(pad_length, dtype=torch.long)])\n",
    "        labels = torch.cat([labels, torch.full((pad_length,), -100)])\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "tokenized_ds_finetune = ds.map(create_tokenized_ds_for_finetune, remove_columns=ds['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length Used: 256\n",
      "\n",
      "Example 1:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?. Options: A: ignore B: enforce C: authoritarian D: yell at E: avoid. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: A\n",
      "Number of Tokens Contributing to Loss: 1\n",
      "Number of Ignored Tokens (Padding or Input): 255\n",
      "\n",
      "Example 2:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: Sammy wanted to go to where the people were.  Where might he go?. Options: A: race track B: populated areas C: the desert D: apartment E: roadblock. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: B\n",
      "Number of Tokens Contributing to Loss: 1\n",
      "Number of Ignored Tokens (Padding or Input): 255\n",
      "\n",
      "Example 3:\n",
      "Input Length: 256\n",
      "Labels Length: 256\n",
      "Input Tokens Decoded: <|begin_of_text|>Question: To locate a choker not located in a jewelry box or boutique where would you go?. Options: A: jewelry store B: neck C: jewlery box D: jewelry box E: boutique. Return only the letter corresponding to the correct answer: A, B, C, D, or E. Answer:<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "Labels Tokens Decoded: A\n",
      "Number of Tokens Contributing to Loss: 1\n",
      "Number of Ignored Tokens (Padding or Input): 255\n"
     ]
    }
   ],
   "source": [
    "# Print the max length used in tokenization\n",
    "print(f\"Max Length Used: {max_length}\")\n",
    "\n",
    "# Check the first 3 examples from the training dataset\n",
    "for idx in range(3):  \n",
    "    example = tokenized_ds_finetune['train'][idx]  \n",
    "    input_ids = example['input_ids']\n",
    "    labels = example['labels']\n",
    "\n",
    "    # Convert to tensors (optional for clarity)\n",
    "    input_ids_tensor = torch.tensor(input_ids)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    # Print token lengths\n",
    "    print(f\"\\nExample {idx + 1}:\")\n",
    "    print(\"Input Length:\", len(input_ids))  \n",
    "    print(\"Labels Length:\", len(labels))\n",
    "\n",
    "    # Decode input and target tokens to check proper alignment\n",
    "    print(\"Input Tokens Decoded:\", tokenizer.decode(input_ids))\n",
    "    print(\"Labels Tokens Decoded:\", tokenizer.decode([t for t in labels if t != -100]))\n",
    "\n",
    "    # Check how many tokens are actually contributing to the loss (non -100)\n",
    "    valid_loss_tokens = len([t for t in labels if t != -100])\n",
    "    print(f\"Number of Tokens Contributing to Loss: {valid_loss_tokens}\")\n",
    "\n",
    "    # Ensure padding tokens are masked properly\n",
    "    num_padding_tokens = labels.count(-100)\n",
    "    print(f\"Number of Ignored Tokens (Padding or Input): {num_padding_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,677,312 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    bias=bias,      \n",
    "    lora_dropout=lora_dropout,     \n",
    "    task_type=TaskType.CAUSAL_LM  \n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_ds_finetune[\"train\"].select(range(training_size))\n",
    "validation_dataset = tokenized_ds_finetune[\"validation\"].select(range(validation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3726600/322336340.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 28:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.948700</td>\n",
       "      <td>7.585327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.414600</td>\n",
       "      <td>1.807538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>1.075743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.267500</td>\n",
       "      <td>2.280303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.129434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.184811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.200341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.419130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.506366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.612757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ktanaka/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned/llama-qa-lora_16/tokenizer_config.json',\n",
       " './fine-tuned/llama-qa-lora_16/special_tokens_map.json',\n",
       " './fine-tuned/llama-qa-lora_16/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))  \n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = \"<|pad|>\"\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<|pad|>\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=trained_folder,     \n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    save_strategy=\"epoch\",           \n",
    "    learning_rate=learning_rate,               \n",
    "    per_device_train_batch_size=train_batch_size,    \n",
    "    per_device_eval_batch_size=eval_batch_size,    \n",
    "    num_train_epochs=train_epochs,\n",
    "    weight_decay=weight_decay,                \n",
    "    logging_dir=\"./logs\",            \n",
    "    logging_steps=1,                \n",
    "    save_total_limit=3,              \n",
    "    load_best_model_at_end=True,     \n",
    "    fp16=is_fp16,                       \n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,   \n",
    "    report_to=\"none\"                 \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(trained_folder)\n",
    "tokenizer.save_pretrained(trained_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0,8,32,\"q_proj, v_proj\",none,0.0,16,16,3e-05,1,1,10,0.01,False,1\n"
     ]
    }
   ],
   "source": [
    "# Convert target_modules list into a quoted CSV-friendly string\n",
    "target_modules_str = '\"' + \", \".join(target_modules) + '\"'  # Joins elements with commas and wraps in quotes\n",
    "\n",
    "# Print all values as a CSV string with target_modules converted properly\n",
    "values_string = \",\".join([\n",
    "    str(llm_int8_threshold),\n",
    "    str(rank),\n",
    "    str(lora_alpha),\n",
    "    target_modules_str,  # Now properly formatted\n",
    "    str(bias),\n",
    "    str(lora_dropout),\n",
    "    str(training_size),\n",
    "    str(validation_size),\n",
    "    str(learning_rate),\n",
    "    str(train_batch_size),\n",
    "    str(eval_batch_size),\n",
    "    str(train_epochs),\n",
    "    str(weight_decay),\n",
    "    str(is_fp16),\n",
    "    str(gradient_accumulation_steps)\n",
    "])\n",
    "\n",
    "# Print CSV values for Google Sheets\n",
    "print(values_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch 0 ===\n",
      "input_ids torch.Size([2, 256])\n",
      "attention_mask torch.Size([2, 256])\n",
      "labels torch.Size([2, 256])\n",
      "\n",
      "=== Batch 1 ===\n",
      "input_ids torch.Size([2, 256])\n",
      "attention_mask torch.Size([2, 256])\n",
      "labels torch.Size([2, 256])\n",
      "\n",
      "=== Batch 2 ===\n",
      "input_ids torch.Size([1, 256])\n",
      "attention_mask torch.Size([1, 256])\n",
      "labels torch.Size([1, 256])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,           # We are doing causal LM, not MLM\n",
    "    pad_to_multiple_of=8 # or None\n",
    ")\n",
    "\n",
    "# Make a small subset to test\n",
    "train_dataset_small = tokenized_ds_finetune['train'].select(range(5))\n",
    "\n",
    "# Create a simple DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_small,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    print(f\"=== Batch {batch_idx} ===\")\n",
    "    for k, v in batch.items():\n",
    "        print(k, v.shape if hasattr(v, 'shape') else type(v))\n",
    "    print()\n",
    "    # Optionally, do a quick forward pass if the model is loaded:\n",
    "    # outputs = model(**batch)\n",
    "    # print(\"Loss:\", outputs.loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
