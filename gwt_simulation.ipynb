{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7904778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, List, Any\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnslothLLM(LLM):\n",
    "    model_name: str\n",
    "    tools: Optional[List[Any]] = None\n",
    "    _model: Any = PrivateAttr()\n",
    "    _tokenizer: Any = PrivateAttr()\n",
    "    _device: str = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        max_seq_length: int,\n",
    "        dtype,\n",
    "        load_in_4bit: bool,\n",
    "        device: str\n",
    "    ):\n",
    "        super().__init__(model_name=model_name)\n",
    "        self.model_name = model_name\n",
    "\n",
    "        print(f\"[INIT] Loading model: {self.model_name}\")\n",
    "        self._model, self._tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit\n",
    "        )\n",
    "        FastLanguageModel.for_inference(self._model)\n",
    "        self._model.to(device)\n",
    "        self._device = device\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response from the model, encouraging an instruction style\n",
    "        to reduce echoing. Uses basic generation params to reduce repetition.\n",
    "        \"\"\"\n",
    "        print(f\"[CALL] Model '{self.model_name}' => prompt: {prompt!r}\")\n",
    "\n",
    "        # System-level directive to reduce echoing\n",
    "        system_msg = \"You are a helpful assistant. Avoid echoing user queries.\\n\\n\"\n",
    "\n",
    "        instruction_prompt = (\n",
    "            f\"{system_msg}\"\n",
    "            f\"### Instruction:\\n{prompt}\\n\\n\"\n",
    "            f\"### Response:\\n\"\n",
    "        )\n",
    "\n",
    "        inputs = self._tokenizer(instruction_prompt, return_tensors=\"pt\").to(self._device)\n",
    "        outputs = self._model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Handle stop tokens if needed\n",
    "        if stop is not None:\n",
    "            for token in stop:\n",
    "                idx = response.find(token)\n",
    "                if idx != -1:\n",
    "                    response = response[:idx]\n",
    "                    break\n",
    "\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"unsloth\"\n",
    "\n",
    "    def bind_tools(self, tools: List[Any]) -> \"UnslothLLM\":\n",
    "        self.tools = tools\n",
    "        return self\n",
    "\n",
    "\n",
    "class ChatUnslothLLM(UnslothLLM):\n",
    "    def invoke(self, input: Any, config: Optional[dict] = None, **kwargs) -> AIMessage:\n",
    "        if isinstance(input, dict) and \"messages\" in input and input[\"messages\"]:\n",
    "            prompt = input[\"messages\"][-1].content\n",
    "        else:\n",
    "            prompt = str(input)\n",
    "\n",
    "        response_text = self._call(prompt, **kwargs)\n",
    "        return AIMessage(content=response_text, name=\"ai\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# \"Hub\" or \"controller\" model\n",
    "base_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "hub_llm = ChatUnslothLLM(\n",
    "    model_name=base_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# LoRA-based psychology model\n",
    "psychology_model_name = \"lora_model_osloth_psychology\"\n",
    "psychology_hub_llm = ChatUnslothLLM(\n",
    "    model_name=psychology_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# LoRA-based logic model\n",
    "logical_model_name = \"lora_model_osloth_commonsense_qa\"\n",
    "logical_hub_llm = ChatUnslothLLM(\n",
    "    model_name=logical_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def psychological_understanding(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Use the LoRA psychology model to provide psychological insights\n",
    "    about the supplied query.\n",
    "    \"\"\"\n",
    "    print(\"[TOOL] psychological_understanding was called with:\", query)\n",
    "    return psychology_hub_llm._call(query)\n",
    "\n",
    "@tool\n",
    "def logical_reasoning(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Use the LoRA commonsense QA model to offer logical reasoning\n",
    "    about the supplied query.\n",
    "    \"\"\"\n",
    "    print(\"[TOOL] logical_reasoning was called with:\", query)\n",
    "    return logical_hub_llm._call(query)\n",
    "\n",
    "def classify_and_toolflow(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    1) Ask the base model to classify the query as 'psychology', 'logic', 'both', or 'none'.\n",
    "    2) If 'psychology', call psychological_understanding.\n",
    "       If 'logic', call logical_reasoning.\n",
    "       If 'both', call both in sequence.\n",
    "    3) Combine the results with the original query, then ask the base model for a final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Classification prompt for the base model\n",
    "    classification_prompt = (\n",
    "        \"The user query is: \"\n",
    "        f\"\\\"{user_query}\\\"\\n\\n\"\n",
    "        \"Decide if this is about psychology, logic, both, or none.\\n\"\n",
    "        \"Reply with exactly one word: 'psychology', 'logic', 'both', or 'none'.\"\n",
    "    )\n",
    "\n",
    "    classification = hub_llm._call(classification_prompt).lower()\n",
    "    print(\"[DEBUG] Classification result:\", classification)\n",
    "\n",
    "    # 2) Based on classification, call the specialized tool(s)\n",
    "    tool_responses = []\n",
    "    if \"psychology\" in classification:\n",
    "        tool_output = psychological_understanding(user_query)\n",
    "        tool_responses.append(f\"Psychology Tool Output:\\n{tool_output}\")\n",
    "    if \"logic\" in classification:\n",
    "        tool_output = logical_reasoning(user_query)\n",
    "        tool_responses.append(f\"Logic Tool Output:\\n{tool_output}\")\n",
    "\n",
    "    # 3) Combine the user query + tool outputs into a final prompt for the base LLM\n",
    "    if len(tool_responses) == 0:\n",
    "        # If 'none', no tool used, just respond directly\n",
    "        final_prompt = (\n",
    "            f\"User asked: \\\"{user_query}\\\"\\n\"\n",
    "            \"No specialized tools were used because classification was 'none'.\\n\\n\"\n",
    "            \"Now please give a final answer to the user.\"\n",
    "        )\n",
    "    else:\n",
    "        combined_tool_text = \"\\n\\n\".join(tool_responses)\n",
    "        final_prompt = (\n",
    "            f\"User asked: \\\"{user_query}\\\"\\n\\n\"\n",
    "            f\"You used the following tool(s) output:\\n{combined_tool_text}\\n\\n\"\n",
    "            \"Now, combine these results into a helpful final answer.\"\n",
    "        )\n",
    "\n",
    "    # 4) The base model returns a final, polished answer\n",
    "    print(\"[Final Answer]: \")\n",
    "    final_answer = hub_llm._call(final_prompt)\n",
    "    return final_answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_query = \"I have a psychological issue. Give me advice.\"\n",
    "    print(\"[USER QUERY]\", user_query)\n",
    "\n",
    "    # Instead of agent.invoke(...), we do our manual classification-based approach\n",
    "    answer = classify_and_toolflow(user_query)\n",
    "\n",
    "    print(\"\\n===== Final Answer =====\")\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
