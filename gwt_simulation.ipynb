{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6675aa-ed47-47fe-bb4f-f7cd923d1a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from typing import Optional, List, Any\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.agents.mrkl.output_parser import MRKLOutputParser, AgentFinish\n",
    "from langchain.agents.agent import OutputParserException\n",
    "from pydantic import PrivateAttr\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1bcac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnslothLLM(LLM):\n",
    "    model_name: str\n",
    "\n",
    "    _model: Any = PrivateAttr()\n",
    "    _tokenizer: Any = PrivateAttr()\n",
    "    _device: str = PrivateAttr()\n",
    "\n",
    "    def __init__(self, model_name: str, max_seq_length: int, dtype, load_in_4bit: bool, device: str):\n",
    "        super().__init__(model_name=model_name)\n",
    "        self.model_name = model_name\n",
    "        self._model, self._tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(self._model)\n",
    "        self._model.to(device)\n",
    "        self._device = device\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        inputs = self._tokenizer(prompt, return_tensors=\"pt\").to(self._device)\n",
    "        outputs = self._model.generate(**inputs, max_new_tokens=256)\n",
    "        response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        return {\"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"unsloth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfe61f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "base_model_name       = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "psychology_model_name = \"lora_model_osloth_psychology\"\n",
    "logical_model_name    = \"lora_model_osloth_commonsense_qa\"\n",
    "\n",
    "hub_llm = UnslothLLM(base_model_name, max_seq_length, dtype, load_in_4bit, device)\n",
    "psychology_llm = UnslothLLM(psychology_model_name, max_seq_length, dtype, load_in_4bit, device)\n",
    "logical_llm = UnslothLLM(logical_model_name, max_seq_length, dtype, load_in_4bit, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd71ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response:\n",
      "the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: You are a helpful assistant that uses tools to answer questions. When you have an answer, output only a final answer in the format:\n",
      "Final Answer: <your answer here>\n",
      "Do not include any intermediate thoughts or actions.I often feel overwhelmed and lost with my emotions. Can you help me understand what might be causing these feelings and suggest a way to cope?\n",
      "Thought: I should use PsychologicalUnderstanding to help me understand what might be causing these feelings.\n",
      "Action: PsychologicalUnderstanding\n",
      "Action Input: I often feel overwhelmed and lost with my emotions. Can you help me understand what might be causing these feelings and suggest a way to cope?\n",
      "Observation: You are feeling overwhelmed and lost with your emotions. This is likely due to a combination of factors, such as a stressful situation, a lack of support, or a difficult decision. It is important to remember that emotions are normal and healthy, and that it is okay to feel overwhelmed. However, it is also important to find ways to cope with these feelings. Some strategies for coping with emotions include talking to a trusted friend or family member, practicing relaxation techniques such as deep breathing or meditation, or seeking professional help if necessary. It is also important to remember that emotions are temporary and will eventually pass. It is important to take care of yourself and find ways to cope that work for you.\n",
      "Thought: I should use PsychologicalUnderstanding to help me understand what might be causing these feelings.\n",
      "Action: PsychologicalUnderstanding\n",
      "Action Input: I often feel overwhelmed and lost with my emotions. Can you help me understand what might be causing these feelings and suggest a way to cope?\n",
      "Observation: You are feeling overwhelmed and lost\n"
     ]
    }
   ],
   "source": [
    "def psychology_tool(query: str) -> str:\n",
    "    return psychology_llm(query)\n",
    "\n",
    "def logical_tool(query: str) -> str:\n",
    "    return logical_llm(query)\n",
    "\n",
    "psychology_tool_instance = Tool(\n",
    "    name=\"PsychologicalUnderstanding\",\n",
    "    func=psychology_tool,\n",
    "    description=(\n",
    "        \"Use this tool for queries requiring psychological insights, empathy, or emotional understanding.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "logical_tool_instance = Tool(\n",
    "    name=\"LogicalReasoning\",\n",
    "    func=logical_tool,\n",
    "    description=(\n",
    "        \"Use this tool for queries requiring logical reasoning, analytical problem solving, and clear analytical thinking.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "tools = [psychology_tool_instance, logical_tool_instance]\n",
    "\n",
    "\n",
    "class CustomOutputParser(MRKLOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        final_match = re.search(r\"Final Answer:\\s*(.*)\", text, re.DOTALL)\n",
    "        if final_match:\n",
    "            final_answer = final_match.group(1).strip()\n",
    "            return AgentFinish({\"output\": final_answer}, text)\n",
    "        try:\n",
    "            return super().parse(text)\n",
    "        except OutputParserException as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    hub_llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False,\n",
    "    agent_kwargs={\"output_parser\": CustomOutputParser()}\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = (\n",
    "        \"You are a helpful assistant that answers questions directly without showing your internal reasoning or chain-of-thought.\\n\"\n",
    "        \"When you have an answer, output it using the exact format:\\n\"\n",
    "        \"Final Answer: <your answer here>\\n\"\n",
    "        \"Do not include any intermediate thoughts or actions.\\n\\n\"\n",
    "        \"I often feel overwhelmed and lost with my emotions. \"\n",
    "        \"Can you help me understand what might be causing these feelings \"\n",
    "        \"and suggest a way to cope?\"\n",
    "    )\n",
    "    response = agent.run(query)\n",
    "    print(\"Final Response:\")\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
