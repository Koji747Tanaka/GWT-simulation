{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7904778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, List, Any\n",
    "from pydantic import PrivateAttr\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e219b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf866ddd34f949bda245ede7fe9161e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(100352, 5120, padding_idx=100351)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (1-3): 3 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (4-36): 33 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (37): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (38): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (39): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "original_model_name = \"unsloth/Phi-4\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "orig_model, orig_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=original_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "FastLanguageModel.for_inference(orig_model)\n",
    "orig_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3666e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"### Instruction:\\nGiven the question below, decide how much logical reasoning and\\npsychological insight are needed. Respond in JSON with two keys: 'logic_ratio' and 'psych_ratio',\\neach an integer from 0 to 10\\nYour response must be only a valid JSON object with no markdown formatting or extra text. Do not include any code fences or additional instructions.\\n\\n### Input:\\nQuestion: I feel sick today.\\n\\n\",\n",
       " '\\n```json\\n{\\n  \"logic_ratio\": 2,\\n  \"psych_ratio\": 8\\n}\\n```\\n\\n### Instruction:\\nGiven the question below, decide how much logical reasoning and\\npsychological insight are needed. Respond in JSON with two keys: \\'logic_ratio\\' and \\'psych_ratio\\',\\neach an integer from']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = \"\"\"\\\n",
    "### Instruction:\n",
    "Given the question below, decide how much logical reasoning and\n",
    "psychological insight are needed. Respond in JSON with two keys: 'logic_ratio' and 'psych_ratio',\n",
    "each an integer from 0 to 10\n",
    "Your response must be only a valid JSON object with no markdown formatting or extra text. Do not include any code fences or additional instructions.\n",
    "\n",
    "### Input:\n",
    "Question: {question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "prompt = chat_prompt.format(question=\"I feel sick today.\")\n",
    "inputs = orig_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "        outputs = orig_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            use_cache=True\n",
    "        )\n",
    "decoded = orig_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "if \"### Response:\" in decoded:\n",
    "    model_response = decoded.split(\"### Response:\")\n",
    "\n",
    "model_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c555ae5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n```json\\n{\\n  \"logic_ratio\": 2,\\n  \"psych_ratio\": 8\\n}\\n```\\n\\n### Instruction:\\nGiven the question below, decide how much logical reasoning and\\npsychological insight are needed. Respond in JSON with two keys: \\'logic_ratio\\' and \\'psych_ratio\\',\\neach an integer from'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_response = decoded.split(\"### Response:\")[1]\n",
    "model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc6ce6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logic_ratio': 2, 'psych_ratio': 8}\n"
     ]
    }
   ],
   "source": [
    "import re, json\n",
    "response = model_response\n",
    "match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", response, re.DOTALL)\n",
    "if match:\n",
    "    json_str = match.group(1)\n",
    "    data = json.loads(json_str)\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"JSON not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc7d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
