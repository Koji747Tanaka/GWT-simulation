{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Assuming you have installed 'unsloth' and 'trl'\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# File that contains the 100 multiple-choice questions\n",
    "TEST_FILE = \"psychology_test_multiple_choice.json\"\n",
    "\n",
    "# Define a multiple-choice chat prompt\n",
    "# The prompt instructs the model to respond with a single correct letter.\n",
    "chat_prompt = \"\"\"\\\n",
    "### Instruction:\n",
    "You are a helpful AI assistant. Read the multiple choice question and options below. \n",
    "Provide only the single best answer (A, B, C, or D). Just giving me an alphabet is enough.\n",
    "Example is this.\n",
    "Response: A, B, C, or D\n",
    "\n",
    "### Input:\n",
    "Question: {question}\n",
    "Options:\n",
    "{options_str}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def load_test_data(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_letter_from_response(text):\n",
    "    # Simple approach: search for an isolated letter A-D\n",
    "    match = re.search(r\"\\b[ABCD]\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return None\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, device=\"cuda\", model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on multiple-choice items.\n",
    "    Returns the accuracy (fraction correct).\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "    invalid_responses = 0\n",
    "    \n",
    "    for item in test_data:\n",
    "        q = item[\"question\"]\n",
    "        opts = \"\\n\".join(item[\"options\"])\n",
    "        true_answer = item[\"answer\"].upper()\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = chat_prompt.format(question=q, options_str=opts)\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded\n",
    "\n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1  # Count invalid responses\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    # print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model...\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Evaluating original model...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'invalid_responses' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     evaluate_model(finetune_model, finetune_tokenizer, test_data, device\u001b[38;5;241m=\u001b[39mdevice, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-Tuned Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Evaluate original model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating original model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOriginal Model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 2) Load the FINE-TUNED model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# This folder \"lora_model_osloth\" is whatever you saved from your fine-tuning.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Make sure it matches your actual path.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading fine-tuned model (LoRA)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 78\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, test_data, device, model_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m predicted_letter \u001b[38;5;241m=\u001b[39m get_letter_from_response(model_response)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_letter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     invalid_responses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Count invalid responses\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m predicted_letter \u001b[38;5;241m==\u001b[39m true_answer:\n\u001b[1;32m     80\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'invalid_responses' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load test data\n",
    "    test_data = load_test_data(TEST_FILE)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(\"Loading original model...\")\n",
    "    original_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "\n",
    "    max_seq_length = 2048\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "\n",
    "    orig_model, orig_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=original_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "    # Prepare it for inference (disable training settings)\n",
    "    FastLanguageModel.for_inference(orig_model)\n",
    "    orig_model.to(device)\n",
    "\n",
    "    # Evaluate original model\n",
    "    print(\"Evaluating original model...\")\n",
    "    evaluate_model(orig_model, orig_tokenizer, test_data, device=device, model_name=\"Original Model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2) Load the FINE-TUNED model\n",
    "    # ----------------------------------------------------------------------\n",
    "    # This folder \"lora_model_osloth\" is whatever you saved from your fine-tuning.\n",
    "    # Make sure it matches your actual path.\n",
    "    print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "    fine_tuned_model_name = \"lora_model_osloth\"\n",
    "\n",
    "    finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=fine_tuned_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "    # Prepare for inference\n",
    "    FastLanguageModel.for_inference(finetune_model)\n",
    "    finetune_model.to(device)\n",
    "\n",
    "    # Evaluate fine-tuned model\n",
    "    print(\"Evaluating fine-tuned model...\")\n",
    "    evaluate_model(finetune_model, finetune_tokenizer, test_data, device=device, model_name=\"Fine-Tuned Model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model...\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_test_data(TEST_FILE)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Loading original model...\")\n",
    "original_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "orig_model, orig_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=original_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(orig_model)\n",
    "orig_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: D\n",
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: D\n",
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: C\n"
     ]
    }
   ],
   "source": [
    "for item in test_data[:30]:\n",
    "    q = item[\"question\"]\n",
    "    opts = \"\\n\".join(item[\"options\"])\n",
    "    true_answer = item[\"answer\"].upper()\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = chat_prompt.format(question=q, options_str=opts)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = orig_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = orig_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            use_cache=True\n",
    "        )\n",
    "    # Decode\n",
    "    decoded = orig_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"### Response:\" in decoded:\n",
    "        model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        model_response = decoded\n",
    "\n",
    "    predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "    if predicted_letter != true_answer:\n",
    "        print(\"==========================\")\n",
    "        print(f\"Correct Answer: {true_answer}\")\n",
    "        print(f\"Predicted letter: {predicted_letter}\")\n",
    "        # print(f\"Decoded Text: {decoded}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating original model...\")\n",
    "evaluate_model(orig_model, orig_tokenizer, test_data, device=device, model_name=\"Original Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading fine-tuned model (LoRA)...\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "fine_tuned_model_name = \"lora_model_osloth\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Prepare for inference\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: B\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: C\n",
      "Predicted letter: B\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: A\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: None\n",
      "==========================\n",
      "Correct Answer: D\n",
      "Predicted letter: A\n"
     ]
    }
   ],
   "source": [
    "for item in test_data[:30]:\n",
    "    q = item[\"question\"]\n",
    "    opts = \"\\n\".join(item[\"options\"])\n",
    "    true_answer = item[\"answer\"].upper()\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = chat_prompt.format(question=q, options_str=opts)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = finetune_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = finetune_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            use_cache=True\n",
    "        )\n",
    "    # Decode\n",
    "    decoded = finetune_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"### Response:\" in decoded:\n",
    "        model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        model_response = decoded\n",
    "\n",
    "    predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "    if predicted_letter != true_answer:\n",
    "        print(\"==========================\")\n",
    "        print(f\"Correct Answer: {true_answer}\")\n",
    "        print(f\"Predicted letter: {predicted_letter}\")\n",
    "        # print(f\"Decoded Text: {decoded}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model\n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "evaluate_model(finetune_model, finetune_tokenizer, test_data, device=device, model_name=\"Fine-Tuned Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
