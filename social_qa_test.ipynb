{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Fix all seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "pd.options.display.max_colwidth = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "socialiqa = load_dataset(\"allenai/social_i_qa\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated prompt including context and three answer choices\n",
    "chat_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Choices:\n",
    "A: {}\n",
    "B: {}\n",
    "C: {}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def load_dataset_validation(dataset_name, split):\n",
    "    \"\"\"Load the validation split of a dataset.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name} [{split}]\")\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset\n",
    "\n",
    "def load_model(model_name, max_seq_length=2048, dtype=None, load_in_4bit=True, device=\"cuda\"):\n",
    "    \"\"\"Load a model and prepare it for inference.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_letter_from_response(text):\n",
    "    # Look for an isolated letter A-C (ignore D and E as SocialIQA has three options)\n",
    "    match = re.search(r\"\\b[ABC]\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return None\n",
    "\n",
    "def get_true_answer(label):\n",
    "    # Convert the string label to int and map to a letter\n",
    "    label_idx = int(label)\n",
    "    mapping = {1: 'A', 2: 'B', 3: 'C'}\n",
    "    return mapping.get(label_idx, None)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    invalid_responses = 0\n",
    "    instruction = \"Answer the multiple-choice question below based on the provided context. Be emphathetic. Your response must be a single letter: A, B, or C.\"\n",
    "    \n",
    "    for i in range(total):\n",
    "        context = dataset[i]['context']\n",
    "        question = dataset[i]['question']\n",
    "        answerA = dataset[i]['answerA']\n",
    "        answerB = dataset[i]['answerB']\n",
    "        answerC = dataset[i]['answerC']\n",
    "        true_answer = get_true_answer(dataset[i]['label'])\n",
    "\n",
    "        prompt = chat_prompt.format(instruction, context, question, answerA, answerB, answerC, \"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded.strip()\n",
    "            \n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "        # print(f\"model response {model_response}\")\n",
    "        # print(f\"predicted_letter: {predicted_letter}\")\n",
    "        # print(f\"true_answer: {true_answer}\")\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def create_psychological_df(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    results = []\n",
    "    \n",
    "    instruction = \"Answer the multiple-choice question below based on the provided context. Be emphathetic. Your response must be a single letter: A, B, or C.\"\n",
    "    \n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        context = dataset[i]['context']\n",
    "        question = dataset[i]['question']\n",
    "        answerA = dataset[i]['answerA']\n",
    "        answerB = dataset[i]['answerB']\n",
    "        answerC = dataset[i]['answerC']\n",
    "        \n",
    "        prompt = chat_prompt.format(instruction, context, question, answerA, answerB, answerC, \"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,  # Increased to capture reasoning\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded.strip()\n",
    "            \n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "\n",
    "        # Extract predicted letter and reasoning\n",
    "        # predicted_letter = get_letter_from_response(decoded)\n",
    "        reasoning = decoded if predicted_letter is None else decoded.replace(predicted_letter, \"\").strip()\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"id\":i,\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answer\": predicted_letter\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logic = pd.read_csv(\"csvs/phi_common_tuned_with_psych_q.csv\")\n",
    "df_psych = pd.read_csv(\"csvs/phi_social_psych_q.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: allenai/social_i_qa [validation]\n"
     ]
    }
   ],
   "source": [
    "validation_data = load_dataset_validation(\"allenai/social_i_qa\", \"validation\")\n",
    "# print(\"validation data 1: \",validation_data)\n",
    "dataset = validation_data.select(range(100))\n",
    "i = 0\n",
    "context = dataset[i]['context']\n",
    "question = dataset[i]['question']\n",
    "answerA = dataset[i]['answerA']\n",
    "answerB = dataset[i]['answerB']\n",
    "answerC = dataset[i]['answerC']\n",
    "true_answer = get_true_answer(dataset[i]['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracy didn't go home that evening and resisted Riley's attacks.\n",
      "Tracy didn't go home that evening and resisted Riley's attacks.\n",
      "Tracy didn't go home that evening and resisted Riley's attacks.\n"
     ]
    }
   ],
   "source": [
    "print(context)\n",
    "print(df_logic.iloc[0][\"context\"])\n",
    "print(df_psych.iloc[0][\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: allenai/social_i_qa [validation]\n",
      "\n",
      "Loading fine-tuned model (LoRA)...\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418ca9274c40415db085b66c7a2211da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load phi_lora_commonsense_qa as a legacy tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Creating psychological df ...\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "# Load test data\n",
    "validation_data = load_dataset_validation(\"allenai/social_i_qa\", \"validation\")\n",
    "# print(\"validation data 1: \",validation_data)\n",
    "validation_data = validation_data.select(range(100))\n",
    "# print(\"validation data 2: \",validation_data)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "\n",
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "fine_tuned_model_name = \"phi_lora_social_qa\"\n",
    "# fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)\n",
    "\n",
    "print(\"\\Creating psychological df ...\")\n",
    "psych_df = create_psychological_df(finetune_model, finetune_tokenizer, validation_data, device=\"cuda\", model_name=\"Fine-Tuned Model\")\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Tracy didn't go home that evening and resisted Riley's attacks.</td>\n",
       "      <td>What does Tracy need to do before this?</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sydney walked past a homeless woman asking for change but did not have any money they could give to her. Sydney felt bad afterwards.</td>\n",
       "      <td>How would you describe Sydney?</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sasha protected the patients' rights by making new laws regarding cancer drug trials.</td>\n",
       "      <td>What will patients want to do next?</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Jordan was in charge of taking the food on the camping trip and left all the food at home.</td>\n",
       "      <td>How would Jordan feel afterwards?</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Kendall opened their mouth to speak and what came out shocked everyone.</td>\n",
       "      <td>How would you describe Kendall?</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>Riley talked to their friends about what they should do that night.</td>\n",
       "      <td>Why did Riley do this?</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>Remy went into town and found a fun place to get treats and ate some ice cream.</td>\n",
       "      <td>What does Remy need to do before this?</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>Carson was at a friend's house playing video games to get away from his arguing parents.</td>\n",
       "      <td>What does Carson need to do before this?</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>Taylor taught math in the schools and got a lot of praise for his style of teaching.</td>\n",
       "      <td>Why did Taylor do this?</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>Robin pulled Carson over for speeding and then didnt write him a ticket.</td>\n",
       "      <td>Why did Robin do this?</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  \\\n",
       "0    0   \n",
       "1    1   \n",
       "2    2   \n",
       "3    3   \n",
       "4    4   \n",
       "..  ..   \n",
       "95  95   \n",
       "96  96   \n",
       "97  97   \n",
       "98  98   \n",
       "99  99   \n",
       "\n",
       "                                                                                                                                 context  \\\n",
       "0                                                                        Tracy didn't go home that evening and resisted Riley's attacks.   \n",
       "1   Sydney walked past a homeless woman asking for change but did not have any money they could give to her. Sydney felt bad afterwards.   \n",
       "2                                                  Sasha protected the patients' rights by making new laws regarding cancer drug trials.   \n",
       "3                                             Jordan was in charge of taking the food on the camping trip and left all the food at home.   \n",
       "4                                                                Kendall opened their mouth to speak and what came out shocked everyone.   \n",
       "..                                                                                                                                   ...   \n",
       "95                                                                   Riley talked to their friends about what they should do that night.   \n",
       "96                                                       Remy went into town and found a fun place to get treats and ate some ice cream.   \n",
       "97                                              Carson was at a friend's house playing video games to get away from his arguing parents.   \n",
       "98                                                  Taylor taught math in the schools and got a lot of praise for his style of teaching.   \n",
       "99                                                              Robin pulled Carson over for speeding and then didnt write him a ticket.   \n",
       "\n",
       "                                    question answer  \n",
       "0    What does Tracy need to do before this?      C  \n",
       "1             How would you describe Sydney?      B  \n",
       "2        What will patients want to do next?      C  \n",
       "3          How would Jordan feel afterwards?      A  \n",
       "4            How would you describe Kendall?      C  \n",
       "..                                       ...    ...  \n",
       "95                    Why did Riley do this?      C  \n",
       "96    What does Remy need to do before this?      C  \n",
       "97  What does Carson need to do before this?      B  \n",
       "98                   Why did Taylor do this?      A  \n",
       "99                    Why did Robin do this?      B  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psych_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_df.to_csv(\"csvs/phi_common_tuned_with_psych_q.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: allenai/social_i_qa [validation]\n",
      "\n",
      "Loading fine-tuned model (LoRA)...\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a9ca0f6d37405d800f1b269dcc3707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load phi_lora_commonsense_qa as a legacy tokenizer.\n",
      "Unsloth 2025.2.12 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating fine-tuned model...\n",
      "[Fine-Tuned Model] Accuracy: 71.00% (71/100)\n",
      "[Fine-Tuned Model] Invalid Responses: 0/100 (0.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load test data\n",
    "validation_data = load_dataset_validation(\"allenai/social_i_qa\", \"validation\")\n",
    "# print(\"validation data 1: \",validation_data)\n",
    "\n",
    "validation_data = validation_data.select(range(100))\n",
    "# print(\"validation data 2: \",validation_data)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "##################\n",
    "# print(\"Loading original model...\")\n",
    "# original_model_name = \"unsloth/Phi-4\"\n",
    "# original_model, original_tokenizer = load_model(\n",
    "#     model_name=original_model_name,\n",
    "#     max_seq_length=max_seq_length,\n",
    "#     load_in_4bit=load_in_4bit,\n",
    "#     device=device,\n",
    "# )\n",
    "# FastLanguageModel.for_inference(original_model)\n",
    "# original_model.to(device)\n",
    "\n",
    "# print(\"\\nEvaluating original model...\")\n",
    "# evaluate_model(original_model, original_tokenizer, validation_data, device=device, model_name=\"Original Model\")\n",
    "#########################\n",
    "# ----------------------------------------------------------------------\n",
    "# 2) Load the FINE-TUNED model\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "fine_tuned_model_name = \"phi_lora_social_qa\"\n",
    "# fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)\n",
    "\n",
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "evaluate_model(finetune_model, finetune_tokenizer, validation_data, device=device, model_name=\"Fine-Tuned Model\")\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
