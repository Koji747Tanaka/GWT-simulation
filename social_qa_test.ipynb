{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Fix all seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "pd.options.display.max_colwidth = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socialiqa = load_dataset(\"allenai/social_i_qa\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Updated prompt including context and three answer choices\n",
    "chat_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Choices:\n",
    "A: {}\n",
    "B: {}\n",
    "C: {}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def load_dataset_validation(dataset_name, split):\n",
    "    \"\"\"Load the validation split of a dataset.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name} [{split}]\")\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset\n",
    "\n",
    "def load_model(model_name, max_seq_length=2048, dtype=None, load_in_4bit=True, device=\"cuda\"):\n",
    "    \"\"\"Load a model and prepare it for inference.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_letter_from_response(text):\n",
    "    # Look for an isolated letter A-C (ignore D and E as SocialIQA has three options)\n",
    "    match = re.search(r\"\\b[ABC]\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return None\n",
    "\n",
    "def get_true_answer(label):\n",
    "    # Convert the string label to int and map to a letter\n",
    "    label_idx = int(label)\n",
    "    mapping = {1: 'A', 2: 'B', 3: 'C'}\n",
    "    return mapping.get(label_idx, None)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    invalid_responses = 0\n",
    "    instruction = \"Answer the multiple-choice question below based on the provided context. Be emphathetic. Your response must be a single letter: A, B, or C.\"\n",
    "    \n",
    "    for i in range(total):\n",
    "        context = dataset[i]['context']\n",
    "        question = dataset[i]['question']\n",
    "        answerA = dataset[i]['answerA']\n",
    "        answerB = dataset[i]['answerB']\n",
    "        answerC = dataset[i]['answerC']\n",
    "        true_answer = get_true_answer(dataset[i]['label'])\n",
    "\n",
    "        prompt = chat_prompt.format(instruction, context, question, answerA, answerB, answerC, \"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded.strip()\n",
    "            \n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "        # print(f\"model response {model_response}\")\n",
    "        # print(f\"predicted_letter: {predicted_letter}\")\n",
    "        # print(f\"true_answer: {true_answer}\")\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def create_psychological_df(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    results = []\n",
    "    \n",
    "    instruction = \"Answer the multiple-choice question below based on the provided context. Be emphathetic. Your response must be a single letter: A, B, or C.\"\n",
    "    \n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        context = dataset[i]['context']\n",
    "        question = dataset[i]['question']\n",
    "        answerA = dataset[i]['answerA']\n",
    "        answerB = dataset[i]['answerB']\n",
    "        answerC = dataset[i]['answerC']\n",
    "        \n",
    "        prompt = chat_prompt.format(instruction, context, question, answerA, answerB, answerC, \"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,  # Increased to capture reasoning\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded.strip()\n",
    "            \n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "\n",
    "        # Extract predicted letter and reasoning\n",
    "        # predicted_letter = get_letter_from_response(decoded)\n",
    "        reasoning = decoded if predicted_letter is None else decoded.replace(predicted_letter, \"\").strip()\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"id\":i,\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answer\": predicted_letter\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Updated prompt including context and three answer choices\n",
    "chat_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Choices:\n",
    "A: {}\n",
    "B: {}\n",
    "C: {}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def load_dataset_validation(dataset_name, split):\n",
    "    \"\"\"Load the validation split of a dataset.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name} [{split}]\")\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset\n",
    "\n",
    "def load_model(model_name, max_seq_length=2048, dtype=None, load_in_4bit=True, device=\"cuda\"):\n",
    "    \"\"\"Load a model and prepare it for inference.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_letter_from_response(text):\n",
    "    # Look for an isolated letter A-C (ignore D and E as SocialIQA has three options)\n",
    "    match = re.search(r\"\\b[ABC]\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return None\n",
    "\n",
    "def get_true_answer(label):\n",
    "    # Convert the string label to int and map to a letter\n",
    "    label_idx = int(label)\n",
    "    mapping = {1: 'A', 2: 'B', 3: 'C'}\n",
    "    return mapping.get(label_idx, None)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    invalid_responses = 0\n",
    "    instruction = \"Answer the multiple-choice question below based on the provided context. Be emphathetic. Your response must be a single letter: A, B, or C.\"\n",
    "    \n",
    "    for i in range(total):\n",
    "        context = dataset[i]['context']\n",
    "        question = dataset[i]['question']\n",
    "        answerA = dataset[i]['answerA']\n",
    "        answerB = dataset[i]['answerB']\n",
    "        answerC = dataset[i]['answerC']\n",
    "        true_answer = get_true_answer(dataset[i]['label'])\n",
    "\n",
    "        prompt = chat_prompt.format(instruction, context, question, answerA, answerB, answerC, \"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded.strip()\n",
    "            \n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "        # print(f\"model response {model_response}\")\n",
    "        # print(f\"predicted_letter: {predicted_letter}\")\n",
    "        # print(f\"true_answer: {true_answer}\")\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_psychological_df_with_logical_knowledge(model, tokenizer, dataset, df_logic, device=\"cuda\", model_name=\"Model\"):\n",
    "    results = []\n",
    "    for i in range(len(dataset)):\n",
    "        context = dataset[i]['context']\n",
    "        question = dataset[i]['question']\n",
    "        answerA = dataset[i]['answerA']\n",
    "        answerB = dataset[i]['answerB']\n",
    "        answerC = dataset[i]['answerC']\n",
    "        logical_exp_answer = df_logic.iloc[i][\"answer\"]\n",
    "        prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Answer the following multiple-choice question based on the context \"\n",
    "        \"Choose only one letter: A, B, or C. Do not explain your answer.\\n\\n\"\n",
    "        f\"### Context:\\n{context}\\n\\n\"\n",
    "        f\"### Question:\\n{question}\\n\\n\"\n",
    "        f\"### Choices:\\n\"\n",
    "        f\"A: {answerA}\\n\"\n",
    "        f\"B: {answerB}\\n\"\n",
    "        f\"C: {answerC}\\n\\n\"\n",
    "        f\"### Thoughts of logical expert:\\n\"\n",
    "        f\"An expert in Logical Reasoning thinks the answer is {logical_exp_answer}.\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "        # prompt = chat_prompt.format(instruction, context, question, answerA, answerB, answerC, \"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,  # Increased to capture reasoning\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded.strip()\n",
    "            \n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "\n",
    "        # Extract predicted letter and reasoning\n",
    "        # predicted_letter = get_letter_from_response(decoded)\n",
    "        reasoning = decoded if predicted_letter is None else decoded.replace(predicted_letter, \"\").strip()\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"id\":i,\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answer\": predicted_letter\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "# Load test data\n",
    "validation_data = load_dataset_validation(\"allenai/social_i_qa\", \"validation\")\n",
    "# print(\"validation data 1: \",validation_data)\n",
    "validation_data = validation_data.select(range(1000))\n",
    "# print(\"validation data 2: \",validation_data)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "\n",
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "fine_tuned_model_name = \"phi_lora_social_qa\"\n",
    "# fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Creating psychological df ...\")\n",
    "psych_df = create_psychological_df(finetune_model, finetune_tokenizer, validation_data, device=\"cuda\", model_name=\"Fine-Tuned Model\")\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_df.to_csv(\"csvs/phi_social_tuned_with_psych_q.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Creating psychological df ...\")\n",
    "psych_df_with_logical_knowledge = create_psychological_df_with_logical_knowledge(finetune_model, finetune_tokenizer, validation_data, device=\"cuda\", model_name=\"Fine-Tuned Model\")\n",
    "\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_df_with_logical_knowledge.to_csv(\"csvs/phi_social_tuned_psych_with_logic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load test data\n",
    "validation_data = load_dataset_validation(\"allenai/social_i_qa\", \"validation\")\n",
    "# print(\"validation data 1: \",validation_data)\n",
    "\n",
    "validation_data = validation_data.select(range(1000))\n",
    "# print(\"validation data 2: \",validation_data)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 64\n",
    "dtype = None\n",
    "load_in_4bit = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading original model...\")\n",
    "original_model_name = \"unsloth/Phi-4\"\n",
    "original_model, original_tokenizer = load_model(\n",
    "    model_name=original_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device=device,\n",
    ")\n",
    "FastLanguageModel.for_inference(original_model)\n",
    "original_model.to(device)\n",
    "\n",
    "print(\"\\nEvaluating original model...\")\n",
    "evaluate_model(original_model, original_tokenizer, validation_data, device=device, model_name=\"Original Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "# fine_tuned_model_name = \"phi_lora_social_qa\"\n",
    "fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)\n",
    "\n",
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "evaluate_model(finetune_model, finetune_tokenizer, validation_data, device=device, model_name=\"Fine-Tuned Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receive answers from two experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_specialized_modules(model, df_psych, df_logic, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    invalid_responses = 0\n",
    "    for i in range(total):\n",
    "        context = dataset[i]['context']\n",
    "        question = dataset[i]['question']\n",
    "        answerA = dataset[i]['answerA']\n",
    "        answerB = dataset[i]['answerB']\n",
    "        answerC = dataset[i]['answerC']\n",
    "        true_answer = get_true_answer(dataset[i]['label'])\n",
    "        psych_exp_answer = df_psych.iloc[i][\"answer\"]\n",
    "        logical_exp_answer = df_logic.iloc[i][\"answer\"]\n",
    "        prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Answer the following multiple-choice question based on the context and the thoughts of two experts. \"\n",
    "        \"Choose only one letter: A, B, or C. Do not explain your answer.\\n\\n\"\n",
    "        f\"### Context:\\n{context}\\n\\n\"\n",
    "        f\"### Question:\\n{question}\\n\\n\"\n",
    "        f\"### Choices:\\n\"\n",
    "        f\"A: {answerA}\\n\"\n",
    "        f\"B: {answerB}\\n\"\n",
    "        f\"C: {answerC}\\n\\n\"\n",
    "        f\"### Thoughts of experts:\\n\"\n",
    "        f\"An expert in Psychological Understanding thinks the answer is {psych_exp_answer}.\\n\"\n",
    "        f\"An expert in Logical Reasoning thinks the answer is {logical_exp_answer}.\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(outputs[0])\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[1].strip()\n",
    "        else:\n",
    "            model_response = decoded.strip()\n",
    "        # print(f\"Model response is {model_response}\")\n",
    "            \n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "        # print(f\"model response {model_response}\")\n",
    "        # print(f\"predicted_letter: {predicted_letter}\")\n",
    "        # print(f\"true_answer: {true_answer}\")\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading original model...\")\n",
    "original_model_name = \"unsloth/Phi-4\"\n",
    "original_model, original_tokenizer = load_model(\n",
    "    model_name=original_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device=device,\n",
    ")\n",
    "FastLanguageModel.for_inference(original_model)\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating original model...\")\n",
    "# evaluate_model(original_model, original_tokenizer, validation_data, device=device, model_name=\"Original Model\")\n",
    "\n",
    "df_psych = pd.read_csv(\"csvs/phi_social_tuned_with_psych_q.csv\")\n",
    "df_logic = pd.read_csv(\"csvs/phi_common_tuned_with_psych_q.csv\")\n",
    "evaluate_model_with_specialized_modules(original_model, df_psych, df_logic, original_tokenizer, validation_data, device=\"cuda\", model_name=\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
