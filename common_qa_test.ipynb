{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Fix all seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "pd.options.display.max_colwidth = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_folder = \"./fine-tuned/lora_model_osloth_commonsense_qa\"\n",
    "ds = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Assuming you have installed 'unsloth' and 'trl'\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "chat_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Choices:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def load_dataset_validation(dataset_name, split):\n",
    "    \"\"\"Load the validation split of a dataset.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name} [{split}]\")\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset\n",
    "\n",
    "def load_model(model_name, max_seq_length=2048, dtype=None, load_in_4bit=True, device=\"cuda\"):\n",
    "    \"\"\"Load a model and prepare it for inference.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_letter_from_response(text):\n",
    "    # Simple approach: search for an isolated letter A-D\n",
    "    match = re.search(r\"\\b[ABCDE]\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return None\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    correct = 0\n",
    "    total = len(dataset['id'])\n",
    "    invalid_responses = 0\n",
    "    instruction = \"Answer the multiple-choice question below based on the provided context. Your response has to be A, B, C, D, or E\"\n",
    "    for i in range(total):\n",
    "        question = dataset['question'][i]\n",
    "        choices = dataset['choices'][i]['text']\n",
    "        labels = dataset['choices'][i]['label']\n",
    "        options_str = \"\\n\".join(f\"{label}: {text}\" for label, text in zip(labels, choices))\n",
    "        true_answer = dataset['answerKey'][i]\n",
    "        prompt = chat_prompt.format(instruction, question, options_str, \"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded\n",
    "        # print(f\"Model response: {model_response}\")\n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "        # print(f\"predicted answer is : {predicted_letter}, true_answer: {true_answer}\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    # print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_logic_df(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    results = []\n",
    "    \n",
    "    instruction = (\n",
    "        \"Read the following multiple-choice question and its context carefully and answer with explanation\"\n",
    "    )\n",
    "    \n",
    "    for i in range(len(dataset['id'])):\n",
    "        question = dataset['question'][i]\n",
    "        choices = dataset['choices'][i]['text']\n",
    "        labels = dataset['choices'][i]['label']\n",
    "        options_str = \"\\n\".join(f\"{label}: {text}\" for label, text in zip(labels, choices))\n",
    "        prompt = chat_prompt.format(instruction, question, options_str, \"\")\n",
    "        \n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        # Decode model response\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded\n",
    "        \n",
    "        # Extract predicted letter and reasoning\n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "        reasoning = model_response.replace(predicted_letter, \"\").strip() if predicted_letter else model_response\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"id\": dataset['id'][i],\n",
    "            \"question\": question,\n",
    "            \"answer\": predicted_letter\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: tau/commonsense_qa [validation]\n",
      "\n",
      "Loading fine-tuned model (LoRA)...\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading fine-tuned model (LoRA)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m fine_tuned_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi_lora_commonsense_qa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m finetune_model, finetune_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfine_tuned_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m FastLanguageModel\u001b[38;5;241m.\u001b[39mfor_inference(finetune_model)\n\u001b[1;32m     26\u001b[0m finetune_model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/osloth/lib/python3.10/site-packages/unsloth/models/loader.py:292\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/osloth/lib/python3.10/site-packages/unsloth/models/llama.py:1774\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[0;32m-> 1774\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1786\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth_zoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1787\u001b[0m         load_vllm,\n\u001b[1;32m   1788\u001b[0m         get_vllm_state_dict,\n\u001b[1;32m   1789\u001b[0m         convert_vllm_to_huggingface,\n\u001b[1;32m   1790\u001b[0m         generate_batches,\n\u001b[1;32m   1791\u001b[0m     )\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/osloth/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/osloth/lib/python3.10/site-packages/transformers/modeling_utils.py:4167\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4164\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   4166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4167\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4170\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/osloth/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load test data\n",
    "validation_data = load_dataset_validation(\"tau/commonsense_qa\", \"validation\")\n",
    "validation_data = validation_data[:100]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)\n",
    "\n",
    "print(\"\\nCreating Df with a fine-tuned model...\")\n",
    "logic_df = create_logic_df(finetune_model, finetune_tokenizer, validation_data, device=\"cuda\", model_name=\"Fine-Tuned Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1afa02df02c908a558b4036e80242fac</td>\n",
       "      <td>A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nA revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\\n\\n### Choices:\\nA: bank\\nB: library\\nC: department store\\nD: mall\\nE: new york\\n\\n### Response:\\nA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a7ab086045575bb497933726e4e6ad28</td>\n",
       "      <td>What do people aim to do at work?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhat do people aim to do at work?\\n\\n### Choices:\\nA: complete job\\nB: learn from each other\\nC: kill animals\\nD: wear hats\\nE: talk to each other\\n\\n### Response:\\nA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b8c0a4703079cf661d7261a60a1bcbff</td>\n",
       "      <td>Where would you find magazines along side many other printed works?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhere would you find magazines along side many other printed works?\\n\\n### Choices:\\nA: doctor\\nB: bookstore\\nC: market\\nD: train station\\nE: mortuary\\n\\n### Response:\\nB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e68fb2448fd74e402aae9982aa76e527</td>\n",
       "      <td>Where are  you likely to find a hamburger?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhere are  you likely to find a hamburger?\\n\\n### Choices:\\nA: fast food restaurant\\nB: pizza\\nC: ground up dead cows\\nD: mouth\\nE: cow carcus\\n\\n### Response:\\nA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2435de612dd69f2012b9e40d6af4ce38</td>\n",
       "      <td>James was looking for a good place to buy farmland.  Where might he look?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nJames was looking for a good place to buy farmland.  Where might he look?\\n\\n### Choices:\\nA: midwest\\nB: countryside\\nC: estate\\nD: farming areas\\nE: illinois\\n\\n### Response:\\nA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5169f7ae0781b15161551de3a189ebef</td>\n",
       "      <td>What do you want someone to do when you illustrate point?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhat do you want someone to do when you illustrate point?\\n\\n### Choices:\\nA: did not understand\\nB: accepting\\nC: make clear\\nD: understood\\nE: understanding\\n\\n### Response:\\nC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ef22ef7aeec70aaa688720f805c1cf38</td>\n",
       "      <td>Billy set aside a block of time for having fun after work. Why might he do this?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nBilly set aside a block of time for having fun after work. Why might he do this?\\n\\n### Choices:\\nA: happiness\\nB: stress relief\\nC: pleasure\\nD: ocean\\nE: may laugh\\n\\n### Response:\\nB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>514310637fb43a252bfadc8cbf79b277</td>\n",
       "      <td>The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nThe man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?\\n\\n### Choices:\\nA: restless\\nB: active\\nC: lazybutt\\nD: productive\\nE: hard work\\n\\n### Response:\\nD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9370b2b0897b796dec4a40f107854c8d</td>\n",
       "      <td>What would you be unable to do if you have too much greed?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhat would you be unable to do if you have too much greed?\\n\\n### Choices:\\nA: keep things\\nB: make friends\\nC: play poker\\nD: conquer opponent\\nE: lie\\n\\n### Response:\\nB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>49902e768c45aa41a0f9f95be81114e5</td>\n",
       "      <td>It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?</td>\n",
       "      <td>\\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nIt was a long trip from the farm, so he stayed in a hotel when he arrived at the what?\\n\\n### Choices:\\nA: bed away from home\\nB: wwii bunker\\nC: resort\\nD: las vegas\\nE: city\\n\\n### Response:\\nE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0   1afa02df02c908a558b4036e80242fac   \n",
       "1   a7ab086045575bb497933726e4e6ad28   \n",
       "2   b8c0a4703079cf661d7261a60a1bcbff   \n",
       "3   e68fb2448fd74e402aae9982aa76e527   \n",
       "4   2435de612dd69f2012b9e40d6af4ce38   \n",
       "..                               ...   \n",
       "95  5169f7ae0781b15161551de3a189ebef   \n",
       "96  ef22ef7aeec70aaa688720f805c1cf38   \n",
       "97  514310637fb43a252bfadc8cbf79b277   \n",
       "98  9370b2b0897b796dec4a40f107854c8d   \n",
       "99  49902e768c45aa41a0f9f95be81114e5   \n",
       "\n",
       "                                                                                                                                 question  \\\n",
       "0                            A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?   \n",
       "1                                                                                                       What do people aim to do at work?   \n",
       "2                                                                     Where would you find magazines along side many other printed works?   \n",
       "3                                                                                              Where are  you likely to find a hamburger?   \n",
       "4                                                               James was looking for a good place to buy farmland.  Where might he look?   \n",
       "..                                                                                                                                    ...   \n",
       "95                                                                              What do you want someone to do when you illustrate point?   \n",
       "96                                                       Billy set aside a block of time for having fun after work. Why might he do this?   \n",
       "97  The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?   \n",
       "98                                                                             What would you be unable to do if you have too much greed?   \n",
       "99                                                 It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                 answer  \n",
       "0                              \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nA revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\\n\\n### Choices:\\nA: bank\\nB: library\\nC: department store\\nD: mall\\nE: new york\\n\\n### Response:\\nA  \n",
       "1                                                                        \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhat do people aim to do at work?\\n\\n### Choices:\\nA: complete job\\nB: learn from each other\\nC: kill animals\\nD: wear hats\\nE: talk to each other\\n\\n### Response:\\nA  \n",
       "2                                                                    \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhere would you find magazines along side many other printed works?\\n\\n### Choices:\\nA: doctor\\nB: bookstore\\nC: market\\nD: train station\\nE: mortuary\\n\\n### Response:\\nB  \n",
       "3                                                                            \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhere are  you likely to find a hamburger?\\n\\n### Choices:\\nA: fast food restaurant\\nB: pizza\\nC: ground up dead cows\\nD: mouth\\nE: cow carcus\\n\\n### Response:\\nA  \n",
       "4                                                           \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nJames was looking for a good place to buy farmland.  Where might he look?\\n\\n### Choices:\\nA: midwest\\nB: countryside\\nC: estate\\nD: farming areas\\nE: illinois\\n\\n### Response:\\nA  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                  ...  \n",
       "95                                                           \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhat do you want someone to do when you illustrate point?\\n\\n### Choices:\\nA: did not understand\\nB: accepting\\nC: make clear\\nD: understood\\nE: understanding\\n\\n### Response:\\nC  \n",
       "96                                                    \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nBilly set aside a block of time for having fun after work. Why might he do this?\\n\\n### Choices:\\nA: happiness\\nB: stress relief\\nC: pleasure\\nD: ocean\\nE: may laugh\\n\\n### Response:\\nB  \n",
       "97  \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nThe man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?\\n\\n### Choices:\\nA: restless\\nB: active\\nC: lazybutt\\nD: productive\\nE: hard work\\n\\n### Response:\\nD  \n",
       "98                                                                  \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nWhat would you be unable to do if you have too much greed?\\n\\n### Choices:\\nA: keep things\\nB: make friends\\nC: play poker\\nD: conquer opponent\\nE: lie\\n\\n### Response:\\nB  \n",
       "99                                          \\n### Instruction:\\nRead the following multiple-choice question and its context carefully and answer with explanation\\n\\n### Question:\\nIt was a long trip from the farm, so he stayed in a hotel when he arrived at the what?\\n\\n### Choices:\\nA: bed away from home\\nB: wwii bunker\\nC: resort\\nD: las vegas\\nE: city\\n\\n### Response:\\nE  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "def main():\n",
    "    # Load test data\n",
    "    validation_data = load_dataset_validation(\"tau/commonsense_qa\", \"validation\")\n",
    "    validation_data = validation_data[:100]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    max_seq_length = 2048\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "##################\n",
    "    # print(\"Loading original model...\")\n",
    "    # original_model_name = \"unsloth/Phi-4\"\n",
    "    # original_model, original_tokenizer = load_model(\n",
    "    #     model_name=original_model_name,\n",
    "    #     max_seq_length=max_seq_length,\n",
    "    #     load_in_4bit=load_in_4bit,\n",
    "    #     device=device,\n",
    "    # )\n",
    "    # FastLanguageModel.for_inference(original_model)\n",
    "    # original_model.to(device)\n",
    "\n",
    "    # print(\"\\nEvaluating original model...\")\n",
    "    # evaluate_model(original_model, original_tokenizer, validation_data, device=device, model_name=\"Original Model\")\n",
    "#########################\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2) Load the FINE-TUNED model\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "    fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "\n",
    "    finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=fine_tuned_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(finetune_model)\n",
    "    finetune_model.to(device)\n",
    "\n",
    "    print(\"\\nEvaluating fine-tuned model...\")\n",
    "    evaluate_model(finetune_model, finetune_tokenizer, validation_data, device=device, model_name=\"Fine-Tuned Model\")\n",
    "########################\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
