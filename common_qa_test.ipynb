{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Fix all seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "pd.options.display.max_colwidth = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_folder = \"./fine-tuned/lora_model_osloth_commonsense_qa\"\n",
    "ds = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Assuming you have installed 'unsloth' and 'trl'\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "chat_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Choices:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def load_dataset_validation(dataset_name, split):\n",
    "    \"\"\"Load the validation split of a dataset.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name} [{split}]\")\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset\n",
    "\n",
    "def load_model(model_name, max_seq_length=2048, dtype=None, load_in_4bit=True, device=\"cuda\"):\n",
    "    \"\"\"Load a model and prepare it for inference.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_letter_from_response(text):\n",
    "    # Simple approach: search for an isolated letter A-D\n",
    "    match = re.search(r\"\\b[ABCDE]\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return None\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    correct = 0\n",
    "    total = len(dataset['id'])\n",
    "    invalid_responses = 0\n",
    "    instruction = \"Answer the multiple-choice question below based on the provided context. Your response has to be A, B, C, D, or E\"\n",
    "    for i in range(total):\n",
    "        question = dataset['question'][i]\n",
    "        choices = dataset['choices'][i]['text']\n",
    "        labels = dataset['choices'][i]['label']\n",
    "        options_str = \"\\n\".join(f\"{label}: {text}\" for label, text in zip(labels, choices))\n",
    "        true_answer = dataset['answerKey'][i]\n",
    "        prompt = chat_prompt.format(instruction, question, options_str, \"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded\n",
    "        # print(f\"Model response: {model_response}\")\n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "        # print(f\"predicted answer is : {predicted_letter}, true_answer: {true_answer}\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    # print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_logic_df(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    results = []\n",
    "    \n",
    "    instruction = (\n",
    "        \"Read the following multiple-choice question and its context carefully and answer with explanation\"\n",
    "    )\n",
    "    \n",
    "    for i in range(len(dataset['id'])):\n",
    "        question = dataset['question'][i]\n",
    "        choices = dataset['choices'][i]['text']\n",
    "        labels = dataset['choices'][i]['label']\n",
    "        options_str = \"\\n\".join(f\"{label}: {text}\" for label, text in zip(labels, choices))\n",
    "        prompt = chat_prompt.format(instruction, question, options_str, \"\")\n",
    "        \n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        # Decode model response\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded\n",
    "        \n",
    "        # Extract predicted letter and reasoning\n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "        reasoning = model_response.replace(predicted_letter, \"\").strip() if predicted_letter else model_response\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"id\": dataset['id'][i],\n",
    "            \"question\": question,\n",
    "            \"answer\": predicted_letter\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: tau/commonsense_qa [validation]\n",
      "\n",
      "Loading fine-tuned model (LoRA)...\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30de9e7d11f6422ab4bf421232d66ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load phi_lora_commonsense_qa as a legacy tokenizer.\n",
      "Unsloth 2025.2.12 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Df with a fine-tuned model...\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load test data\n",
    "validation_data = load_dataset_validation(\"tau/commonsense_qa\", \"validation\")\n",
    "validation_data = validation_data[:100]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "# fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "fine_tuned_model_name = \"phi_lora_social_qa\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)\n",
    "\n",
    "print(\"\\nCreating Df with a fine-tuned model...\")\n",
    "logic_df = create_logic_df(finetune_model, finetune_tokenizer, validation_data, device=\"cuda\", model_name=\"Fine-Tuned Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1afa02df02c908a558b4036e80242fac</td>\n",
       "      <td>A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a7ab086045575bb497933726e4e6ad28</td>\n",
       "      <td>What do people aim to do at work?</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b8c0a4703079cf661d7261a60a1bcbff</td>\n",
       "      <td>Where would you find magazines along side many other printed works?</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e68fb2448fd74e402aae9982aa76e527</td>\n",
       "      <td>Where are  you likely to find a hamburger?</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2435de612dd69f2012b9e40d6af4ce38</td>\n",
       "      <td>James was looking for a good place to buy farmland.  Where might he look?</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5169f7ae0781b15161551de3a189ebef</td>\n",
       "      <td>What do you want someone to do when you illustrate point?</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ef22ef7aeec70aaa688720f805c1cf38</td>\n",
       "      <td>Billy set aside a block of time for having fun after work. Why might he do this?</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>514310637fb43a252bfadc8cbf79b277</td>\n",
       "      <td>The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9370b2b0897b796dec4a40f107854c8d</td>\n",
       "      <td>What would you be unable to do if you have too much greed?</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>49902e768c45aa41a0f9f95be81114e5</td>\n",
       "      <td>It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0   1afa02df02c908a558b4036e80242fac   \n",
       "1   a7ab086045575bb497933726e4e6ad28   \n",
       "2   b8c0a4703079cf661d7261a60a1bcbff   \n",
       "3   e68fb2448fd74e402aae9982aa76e527   \n",
       "4   2435de612dd69f2012b9e40d6af4ce38   \n",
       "..                               ...   \n",
       "95  5169f7ae0781b15161551de3a189ebef   \n",
       "96  ef22ef7aeec70aaa688720f805c1cf38   \n",
       "97  514310637fb43a252bfadc8cbf79b277   \n",
       "98  9370b2b0897b796dec4a40f107854c8d   \n",
       "99  49902e768c45aa41a0f9f95be81114e5   \n",
       "\n",
       "                                                                                                                                 question  \\\n",
       "0                            A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?   \n",
       "1                                                                                                       What do people aim to do at work?   \n",
       "2                                                                     Where would you find magazines along side many other printed works?   \n",
       "3                                                                                              Where are  you likely to find a hamburger?   \n",
       "4                                                               James was looking for a good place to buy farmland.  Where might he look?   \n",
       "..                                                                                                                                    ...   \n",
       "95                                                                              What do you want someone to do when you illustrate point?   \n",
       "96                                                       Billy set aside a block of time for having fun after work. Why might he do this?   \n",
       "97  The man in the white suit was very lazy.  He did nothing useful.  Meanwhile, the ban in the blue had put in effort and was very what?   \n",
       "98                                                                             What would you be unable to do if you have too much greed?   \n",
       "99                                                 It was a long trip from the farm, so he stayed in a hotel when he arrived at the what?   \n",
       "\n",
       "   answer  \n",
       "0       A  \n",
       "1       A  \n",
       "2       B  \n",
       "3       A  \n",
       "4       A  \n",
       "..    ...  \n",
       "95      C  \n",
       "96      B  \n",
       "97      D  \n",
       "98      B  \n",
       "99      E  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_df.to_csv(\"csvs/phi_common_logic_q.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: tau/commonsense_qa [validation]\n",
      "\n",
      "Loading fine-tuned model (LoRA)...\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff1636ebde040d79dc795334c7ae929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load phi_lora_social_qa as a legacy tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating fine-tuned model...\n",
      "[Fine-Tuned Model] Accuracy: 76.00%  (76/100)\n",
      "[Fine-Tuned Model] Invalid Responses: 0/100 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "def main():\n",
    "    # Load test data\n",
    "    validation_data = load_dataset_validation(\"tau/commonsense_qa\", \"validation\")\n",
    "    validation_data = validation_data[:100]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    max_seq_length = 2048\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "##################\n",
    "    # print(\"Loading original model...\")\n",
    "    # original_model_name = \"unsloth/Phi-4\"\n",
    "    # original_model, original_tokenizer = load_model(\n",
    "    #     model_name=original_model_name,\n",
    "    #     max_seq_length=max_seq_length,\n",
    "    #     load_in_4bit=load_in_4bit,\n",
    "    #     device=device,\n",
    "    # )\n",
    "    # FastLanguageModel.for_inference(original_model)\n",
    "    # original_model.to(device)\n",
    "\n",
    "    # print(\"\\nEvaluating original model...\")\n",
    "    # evaluate_model(original_model, original_tokenizer, validation_data, device=device, model_name=\"Original Model\")\n",
    "#########################\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2) Load the FINE-TUNED model\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "    # fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "    fine_tuned_model_name = \"phi_lora_social_qa\"\n",
    "    \n",
    "    finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=fine_tuned_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(finetune_model)\n",
    "    finetune_model.to(device)\n",
    "\n",
    "    print(\"\\nEvaluating fine-tuned model...\")\n",
    "    evaluate_model(finetune_model, finetune_tokenizer, validation_data, device=device, model_name=\"Fine-Tuned Model\")\n",
    "########################\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: tau/commonsense_qa [validation]\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load test data\n",
    "validation_data = load_dataset_validation(\"tau/commonsense_qa\", \"validation\")\n",
    "validation_data = validation_data[:100]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Original Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading original model...\")\n",
    "original_model_name = \"unsloth/Phi-4\"\n",
    "original_model, original_tokenizer = load_model(\n",
    "    model_name=original_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device=device,\n",
    ")\n",
    "FastLanguageModel.for_inference(original_model)\n",
    "original_model.to(device)\n",
    "\n",
    "print(\"\\nEvaluating original model...\")\n",
    "evaluate_model(original_model, original_tokenizer, validation_data, device=device, model_name=\"Original Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "# fine_tuned_model_name = \"phi_lora_commonsense_qa\"\n",
    "fine_tuned_model_name = \"phi_lora_social_qa\"\n",
    "\n",
    "finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=fine_tuned_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetune_model)\n",
    "finetune_model.to(device)\n",
    "\n",
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "evaluate_model(finetune_model, finetune_tokenizer, validation_data, device=device, model_name=\"Fine-Tuned Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receive answers from two experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_specialized_modules(model,  df_psych, df_logic, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    correct = 0\n",
    "    total = len(dataset['id'])\n",
    "    invalid_responses = 0\n",
    "    for i in range(total):\n",
    "        question = dataset['question'][i]\n",
    "        choices = dataset['choices'][i]['text']\n",
    "        labels = dataset['choices'][i]['label']\n",
    "        options_str = \"\\n\".join(f\"{label}: {text}\" for label, text in zip(labels, choices))\n",
    "        true_answer = dataset['answerKey'][i]\n",
    "\n",
    "        psych_exp_answer = df_psych.iloc[i][\"answer\"]\n",
    "        logical_exp_answer = df_logic.iloc[i][\"answer\"]\n",
    "\n",
    "        prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Answer the multiple-choice question below based on the provided context and the thoughts of two experts.\"\n",
    "        \"Your response has to be A, B, C, D, or E\\n\\n\"\n",
    "        f\"### Question:\\n{question}\\n\\n\"\n",
    "        f\"### Choices:\\n\"\n",
    "        f\"{options_str}\\n\\n\"\n",
    "        f\"### Thoughts of experts:\\n\"\n",
    "        f\"An expert in Psychological Understanding thinks the answer is {psych_exp_answer}.\\n\"\n",
    "        f\"An expert in Logical Reasoning thinks the answer is {logical_exp_answer}.\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "        \n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded\n",
    "        # print(f\"Model response: {model_response}\")\n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "        # print(f\"predicted answer is : {predicted_letter}, true_answer: {true_answer}\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    # print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model...\n",
      "Loading model: unsloth/Phi-4\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd157fb318ec4f4692378d2ff9288600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(100352, 5120, padding_idx=100351)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (1-3): 3 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (4-36): 33 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (37): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (38): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "      (39): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=1280, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=17920, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=17920, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading original model...\")\n",
    "original_model_name = \"unsloth/Phi-4\"\n",
    "original_model, original_tokenizer = load_model(\n",
    "    model_name=original_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device=device,\n",
    ")\n",
    "FastLanguageModel.for_inference(original_model)\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating original model...\n",
      "[Model] Accuracy: 90.00%  (9/10)\n",
      "[Model] Invalid Responses: 0/10 (0.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEvaluating original model...\")\n",
    "# evaluate_model(original_model, original_tokenizer, validation_data, device=device, model_name=\"Original Model\")\n",
    "df_psych = pd.read_csv(\"csvs/phi_social_tuned_logic_q.csv\")\n",
    "df_logic = pd.read_csv(\"csvs/phi_common_logic_q.csv\")\n",
    "evaluate_model_with_specialized_modules(original_model, df_psych, df_logic, original_tokenizer, validation_data, device=\"cuda\", model_name=\"Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
