{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils import preprocess_qa, RestrictToValidTokens\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Fix all seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "pd.options.display.max_colwidth = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder = \"./fine-tuned/lora_model_osloth_commonsense_qa\"\n",
    "ds = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ['A', 'B', 'C', 'D', 'E'],\n",
       " 'text': ['bank', 'library', 'department store', 'mall', 'new york']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['validation']['choices'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Assuming you have installed 'unsloth' and 'trl'\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "chat_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Choices:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def get_letter_from_response(text):\n",
    "    # Simple approach: search for an isolated letter A-D\n",
    "    match = re.search(r\"\\b[ABCDE]\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return None\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\", model_name=\"Model\"):\n",
    "    correct = 0\n",
    "    total = len(dataset['id'])\n",
    "    invalid_responses = 0\n",
    "    instruction = \"Answer the multiple-choice question below based on the provided context.\"\n",
    "    for i in range(total):\n",
    "        question = dataset['question'][i]\n",
    "        choices = dataset['choices'][i]['text']\n",
    "        labels = dataset['choices'][i]['label']\n",
    "        options_str = \"\\n\".join(f\"{label}: {text}\" for label, text in zip(labels, choices))\n",
    "        true_answer = dataset['answerKey'][i]\n",
    "\n",
    "        prompt = chat_prompt.format(instruction, question, options_str, \"\")\n",
    "\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"### Response:\" in decoded:\n",
    "            model_response = decoded.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            model_response = decoded\n",
    "\n",
    "        predicted_letter = get_letter_from_response(model_response)\n",
    "\n",
    "        if predicted_letter is None:\n",
    "            invalid_responses += 1\n",
    "        elif predicted_letter == true_answer:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    print(f\"[{model_name}] Invalid Responses: {invalid_responses}/{total} ({invalid_responses/total:.2%})\")\n",
    "    # print(f\"[{model_name}] Accuracy: {accuracy:.2%}  ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: tau/commonsense_qa [validation]\n",
      "Loading original model...\n",
      "Loading model: unsloth/llama-3-8b-bnb-4bit\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.48.0.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "Evaluating original model...\n",
      "[Original Model] Accuracy: 48.16%  (588/1221)\n",
      "[Original Model] Invalid Responses: 20/1221 (1.64%)\n",
      "\n",
      "Loading fine-tuned model (LoRA)...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'finetune_model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m     evaluate_model(finetune_model, finetune_tokenizer, validation_data, device\u001b[38;5;241m=\u001b[39mdevice, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-Tuned Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 62\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading fine-tuned model (LoRA)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Prepare for inference\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m FastLanguageModel\u001b[38;5;241m.\u001b[39mfor_inference(\u001b[43mfinetune_model\u001b[49m)\n\u001b[1;32m     63\u001b[0m finetune_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     65\u001b[0m finetune_model, finetune_tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     66\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mfine_tuned_model_name,\n\u001b[1;32m     67\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[1;32m     68\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m     69\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39mload_in_4bit,\n\u001b[1;32m     70\u001b[0m )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'finetune_model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_dataset_validation(dataset_name, split):\n",
    "    \"\"\"Load the validation split of a dataset.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name} [{split}]\")\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset\n",
    "\n",
    "def load_model(model_name, max_seq_length=2048, dtype=None, load_in_4bit=True, device=\"cuda\"):\n",
    "    \"\"\"Load a model and prepare it for inference.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def main():\n",
    "    # Load test data\n",
    "    validation_data = load_dataset_validation(\"tau/commonsense_qa\", \"validation\")\n",
    "    validation_data = validation_data[:100]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(\"Loading original model...\")\n",
    "    original_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "\n",
    "    max_seq_length = 2048\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "\n",
    "    original_model, original_tokenizer = load_model(\n",
    "        model_name=original_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device=device,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(original_model)\n",
    "    original_model.to(device)\n",
    "\n",
    "    print(\"\\nEvaluating original model...\")\n",
    "    evaluate_model(original_model, original_tokenizer, validation_data, device=device, model_name=\"Original Model\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2) Load the FINE-TUNED model\n",
    "    # ----------------------------------------------------------------------\n",
    "    # This folder \"lora_model_osloth\" is whatever you saved from your fine-tuning.\n",
    "    # Make sure it matches your actual path.\n",
    "    fine_tuned_model_name = \"lora_model_osloth_commonsense_qa\"\n",
    "    print(\"\\nLoading fine-tuned model (LoRA)...\")\n",
    "\n",
    "    # Prepare for inference\n",
    "    FastLanguageModel.for_inference(finetune_model)\n",
    "    finetune_model.to(device)\n",
    "\n",
    "    finetune_model, finetune_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=fine_tuned_model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating fine-tuned model...\")\n",
    "    evaluate_model(finetune_model, finetune_tokenizer, validation_data, device=device, model_name=\"Fine-Tuned Model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
