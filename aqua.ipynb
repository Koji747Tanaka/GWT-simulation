{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import os\n",
    "import bitsandbytes as bnb  # For 8-bit quantization\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "pd.options.display.max_colwidth = None\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "hf_auth_token = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only before fine-tuning or testing original model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,  # Enable 8-bit quantization\n",
    "#     llm_int8_threshold=6.0  # Adjust threshold for higher precision on sensitive layers\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     use_auth_token=hf_auth_token,\n",
    "#     cache_dir=\"/fs03/yu60/kojitanaka/model_cache\"\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     use_auth_token=hf_auth_token,\n",
    "#     cache_dir=\"/fs03/yu60/kojitanaka/model_cache\",\n",
    "#     device_map=\"auto\",  # Automatically maps layers to GPU\n",
    "#     quantization_config=bnb_config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only testing fine-tuned ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a0c9429fb84467b0f93dc873295bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-lora_aqua/aqua_lora24_model_6000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load the base model (this should match the model architecture used in fine-tuning)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fs03/yu60/kojitanaka/model_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Automatically maps layers to GPU\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply the LoRA adapters\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-lora_aqua/aqua_lora24_model_6000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/transformers/modeling_utils.py:4264\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4254\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4257\u001b[0m         (\n\u001b[1;32m   4258\u001b[0m             model,\n\u001b[1;32m   4259\u001b[0m             missing_keys,\n\u001b[1;32m   4260\u001b[0m             unexpected_keys,\n\u001b[1;32m   4261\u001b[0m             mismatched_keys,\n\u001b[1;32m   4262\u001b[0m             offload_index,\n\u001b[1;32m   4263\u001b[0m             error_msgs,\n\u001b[0;32m-> 4264\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4268\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4271\u001b[0m \u001b[43m            \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4272\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4275\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4276\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4284\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4285\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/transformers/modeling_utils.py:4777\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4773\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4774\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4775\u001b[0m                 )\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4777\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4778\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4779\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4780\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4781\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4782\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4783\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4784\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4791\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4793\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4795\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/yu60_scratch/kojitanaka/llama_env/lib64/python3.9/site-packages/transformers/modeling_utils.py:886\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    884\u001b[0m             set_module_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 886\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model, and which\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# uses `param.copy_(input_param)` that preserves the contiguity of the parameter in the model.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/blob/db79ceb110f6646523019a59bbd7b838f43d4a86/torch/nn/modules/module.py#L2040C29-L2040C29\u001b[39;00m\n\u001b[1;32m    892\u001b[0m old_param \u001b[38;5;241m=\u001b[39m model\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=6.0  # Adjust threshold for higher precision on sensitive layers\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llama-lora_aqua/aqua_lora24_model_6000\")\n",
    "\n",
    "# Load the base model (this should match the model architecture used in fine-tuning)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    use_auth_token=hf_auth_token,\n",
    "    cache_dir=\"/fs03/yu60/kojitanaka/model_cache\",\n",
    "    device_map=\"auto\",  # Automatically maps layers to GPU\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Apply the LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, \"llama-lora_aqua/aqua_lora24_model_6000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqua_rat = load_dataset(\"deepmind/aqua_rat\", \"raw\")\n",
    "aqua_df = pd.DataFrame(aqua_rat['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqua_df.columns\n",
    "def preprocess_aqua(example):\n",
    "    return {\n",
    "        \"text\": (\n",
    "            f\"Question: {example['question']}. \"\n",
    "            f\"Options: {example['options']}. \"\n",
    "            \"Please strictly follow this format: \"\n",
    "            \"Explanation: [Your explanation], Answer: [A/B/C/D/E].\"\n",
    "            # f\"Question: {example['question']}. \"\n",
    "            # f\"Options: {example['options']}. \"\n",
    "            # \"Please provide your answer in the exactly the same format as the example 1. Please keep the explanation short.\"\n",
    "            # \"Format is Explanation: your explanation, Answer: [A/B/C/D/E]\"\n",
    "            # \"Example 1:\"\n",
    "            # \"'Explanation: [1 + 1 = 2. Therefore, the answer is A], Answer: [A]'<|eot_id|>.\"\n",
    "            # \"Please note that you have to say Answer: before <|eot_id|>\"\n",
    "            \"Your turn \"\n",
    "            \"Explanation: \"\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_aqua = aqua_rat.map(preprocess_aqua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_aqua['train']\n",
    "# sample_text = tokenized_aqua['train']['text'][0]\n",
    "# print(tokenizer.eos_token_id)\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "eos_token = tokenizer.decode([eos_token_id])\n",
    "print(f\"Decoded EOS Token: {eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def evaluate_explanations(predicted_explanations, correct_explanations):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    bleu_scores = []\n",
    "\n",
    "    for pred, correct in zip(predicted_explanations, correct_explanations):\n",
    "        if not pred or not correct:\n",
    "            continue  # Skip if either explanation is missing\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge = scorer.score(correct, pred)\n",
    "        rouge_scores['rouge1'].append(rouge['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(rouge['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(rouge['rougeL'].fmeasure)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        reference = correct.split()\n",
    "        candidate = pred.split()\n",
    "        bleu_scores.append(sentence_bleu([reference], candidate))\n",
    "\n",
    "    # Average Scores\n",
    "    avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1']) if rouge_scores['rouge1'] else 0\n",
    "    avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2']) if rouge_scores['rouge2'] else 0\n",
    "    avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL']) if rouge_scores['rougeL'] else 0\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "    return avg_rouge1, avg_rouge2, avg_rougeL, avg_bleu\n",
    "\n",
    "def extract_last_answer(text):\n",
    "    # Regex pattern to match \"Answer:\" and capture the alphabet in different formats\n",
    "    pattern = r\"Answer:\\s*[\\[\\(]?([A-E])[\\]\\)]?[.)]?\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)  # Extract the captured alphabet\n",
    "    return None \n",
    "\n",
    "def extract_explanation(prediction_text):\n",
    "    # Find the first occurrence of 'Answer:'\n",
    "    answer_index = prediction_text.find(\"Answer:\")\n",
    "    if answer_index != -1:\n",
    "        return prediction_text[:answer_index].strip()  # Return everything before 'Answer:'\n",
    "    \n",
    "    # If 'Answer:' is not found, return the full text\n",
    "    return prediction_text.strip()\n",
    "\n",
    "def remove_repetition(text, question):\n",
    "    # Normalize whitespace and ensure consistent formatting\n",
    "    question = question.strip()\n",
    "    text = text.strip()\n",
    "    # Remove the question if it appears in the generated text\n",
    "    cleaned_text = text.replace(question, \"\").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def calculate_exact_match(predictions, correct_answers):\n",
    "    correct = sum([1 for pred, correct in zip(predictions, correct_answers) if pred == correct])\n",
    "    return (correct / len(correct_answers)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = tokenized_aqua['test']['text']\n",
    "\n",
    "size = 100\n",
    "for i in range(0, size):\n",
    "    sample_text = sample_texts[i]\n",
    "    inputs = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=400, eos_token_id=tokenizer.eos_token_id, top_k=1)\n",
    "        prediction_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        print(f\"Prediction text: {prediction_text}\")\n",
    "        answer_only = remove_repetition(prediction_text, sample_text)\n",
    "        answer = extract_last_answer(answer_only)\n",
    "        print(f\"Extracted Answer: {answer_only}\")\n",
    "        explanation = extract_explanation(answer_only)\n",
    "\n",
    "        aqua_df.loc[i, 'prediction'] = answer\n",
    "        aqua_df.loc[i, 'predicted_explanation'] = explanation\n",
    "        \n",
    "        print(f\"Progress: {i} / {size}\")\n",
    "        print(f\"Prediction: {answer} Answer: {aqua_df.loc[i]['correct']}\")\n",
    "    \n",
    "predictions = aqua_df['prediction'][0:size].tolist()  # Your model predictions\n",
    "correct_answers = aqua_df['correct'][0:size].tolist()  # Ground truth answers\n",
    "print(f\"predictions: {predictions}\")\n",
    "print(f\"Correct answers: {correct_answers}\")\n",
    "\n",
    "# Evaluate\n",
    "accuracy = calculate_exact_match(predictions, correct_answers)\n",
    "print(f\"Accuracy (Exact Match): {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without fine-tuning: 26.00%\n",
    "# Accuracy fine-tuned (500): 26.00%\n",
    "# Accuracy fine-tuned (5000): 9.00% mostly just None\n",
    "# Fine-tuned 24 with projection layer(1000): 26.00%\n",
    "# Accuracy fine-tuned (6000): 9.00% mostly just None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_explanations = aqua_df['predicted_explanation'][0:size].tolist()  # Predicted explanations\n",
    "correct_explanations = aqua_df['rationale'][0:size].tolist()\n",
    "# Example Usage\n",
    "rouge1, rouge2, rougeL, bleu = evaluate_explanations(predicted_explanations, correct_explanations)\n",
    "print(f\"ROUGE-1: {rouge1:.2f}, ROUGE-2: {rouge2:.2f}, ROUGE-L: {rougeL:.2f}, BLEU: {bleu:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import DatasetDict\n",
    "# Prepare train and validation datasets\n",
    "def preprocess_aqua(example):\n",
    "    return {\n",
    "        \"input_text\": (\n",
    "            f\"Question: {example['question']}. \"\n",
    "            f\"Options: {example['options']}. \"\n",
    "            \"Please strictly follow this format: \"\n",
    "            \"Explanation: [Your explanation], Answer: [A/B/C/D/E].\"\n",
    "        ),\n",
    "        \"target_text\": (\n",
    "            f\"Question: {example['question']}. \"\n",
    "            f\"Options: {example['options']}. \"\n",
    "            \"Please strictly follow this format: \"\n",
    "            \"Explanation: [Your explanation], Answer: [A/B/C/D/E].\"\n",
    "            \"Response:\"\n",
    "            f\"Explanation: {example['rationale']}, Answer: [{example['correct']}]. \"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_aqua = aqua_rat.map(preprocess_aqua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_finetuning(example):\n",
    "    input_encodings = tokenizer(example[\"input_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    target_encodings = tokenizer(example[\"target_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Combine inputs and labels\n",
    "    input_encodings[\"labels\"] = target_encodings[\"input_ids\"]\n",
    "    return input_encodings\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_datasets = processed_aqua.map(tokenize_for_finetuning, batched=True)\n",
    "\n",
    "tokenized_datasets.save_to_disk(\"tokenized_aqua\")\n",
    "# Remove unnecessary columns\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"input_text\", \"target_text\", \"rationale\", \"correct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "tokenized_datasets = load_from_disk(\"tokenized_aqua\")\n",
    "train_subset = tokenized_datasets[\"train\"].select(range(2000))\n",
    "# train_subset = tokenized_datasets[\"train\"]\n",
    "\n",
    "# Replace the original train dataset with the subset (optional)\n",
    "tokenized_datasets[\"train\"] = train_subset\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=16,  # Reduce scaling for stable updates\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    bias=\"none\",  # Bias type (\"none\", \"all\", or \"lora_only\")\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for the model\n",
    ")\n",
    "\n",
    "# Add LoRA adapters to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-llama-lora\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,  # Adjust logging frequency for smaller dataset\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    label_smoothing_factor=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"llama-lora_aqua/aqua_lora16_model_6000\")\n",
    "tokenizer.save_pretrained(\"llama-lora_aqua/aqua_lora16_model_6000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
